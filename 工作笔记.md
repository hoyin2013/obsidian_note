# 2023-03-02

```
# widows 安装wsl所依赖的 ubuntu linux子系统
Invoke-WebReque[[README]]st -Uri https://aka.ms/wslubuntu2004 -OutFile Ubuntu.appx -UseBasicParsing
# 进入到  PS C:\Windows\system32> 目录下
Add-AppxPackage .\Ubuntu.appx

# 点击  Ubuntu.appx 出现启动界面 点击启动出现错误，于是，在powershell中执行下面的命令并安装 wsl_update_x64.msi 安装包

bcdedit /set hypervisorlaunchtype Auto
Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Hyper-V -All
Enable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform
Enable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux

# 安装 wsl2
https://learn.microsoft.com/zh-cn/windows/wsl/install-manual#step-4---download-the-linux-kernel-update-package
https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi　
```

```
# 配置日志输出
import logging

logging.basicConfig(filename='loggmsg.log', level=logging.DEBUG,
                    format='%(asctime)s %(levelname)s [%(lineno)d] %(message)s',
                    encoding='utf-8'
                    )

logging.info('haha')

%(levelno)s: 打印日志级别的数值
%(levelname)s: 打印日志级别名称
%(pathname)s: 打印当前执行程序的路径，其实就是sys.argv[0]
%(filename)s: `打印当前执行程序名`
%(funcName)s: 打印日志的当前函数
%(lineno)d: 打印日志的当前行号
%(asctime)s: 打印日志的时间
%(thread)d: 打印线程ID
%(threadName)s: 打印线程名称
%(process)d: 打印进程ID
%(message)s: 打印日志信息
```

# 2023-03-03

```
# windows 安装 scoop
# 在powershell（管理员模式）下执行命令
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
iwr -useb get.scoop.sh | iex
iwr : 未能解析此远程名称: 'raw.githubusercontent.com'  估计是不能访问这个网站

# 10.80.16.91 行业ocean库延迟问题持续观察
```

# ‍2023-03-06

```
mysql 兼容列表
https://www.mysql.com/support/supportedplatforms/database.html

curl -s -XPOST -H 'Content-Type:application/json-rpc' -d '' http://10.83.5.32/zabbix/api_jsonrpc.php

http://10.80.3.47:9090/api/v1/query?query=mysql_slave_status_seconds_behind_master

# zabbix docker安装，注意启动完以后要等待十几分钟以后才能登陆
# mysql8
docker run --name mysql-server -t \
      -e MYSQL_USER="zabbix" \
      -e MYSQL_PASSWORD="zabbix" \
      -e MYSQL_ROOT_PASSWORD="zabbix" \
      -p 3306:3306 \
      -d mysql  \
      --character-set-server=utf8 --collation-server=utf8_bin
# zabbix-java-gateway
docker run --name zabbix-java-gateway -t -d zabbix/zabbix-java-gateway:latest

# zabbix-server-mysql
docker run --name zabbix-server-mysql -t \
      -e DB_SERVER_HOST="mysql-server" \
      -e MYSQL_DATABASE="zabbix" \
      -e MYSQL_USER="zabbix" \
      -e MYSQL_PASSWORD="zabbix" \
      -e MYSQL_ROOT_PASSWORD="zabbix" \
      -e ZBX_JAVAGATEWAY="zabbix-java-gateway" \
      --link mysql-server:mysql \
      --link zabbix-java-gateway:zabbix-java-gateway \
      -p 10051:10051 \
      -d zabbix/zabbix-server-mysql:latest

# zabbix-web-nginx-mysql
docker run --name zabbix-web-nginx-mysql -t \
      -e DB_SERVER_HOST="mysql-server" \
      -e MYSQL_DATABASE="zabbix" \
      -e MYSQL_USER="zabbix" \
      -e MYSQL_PASSWORD="zabbix" \
      -e MYSQL_ROOT_PASSWORD="zabbix" \
      --link mysql-server:mysql \
      --link zabbix-server-mysql:zabbix-server \
      -p 80:8080 \
      -d zabbix/zabbix-web-nginx-mysql:latest

# zabbix-agent
docker run --name zabbix-agent \
      -e ZBX_HOSTNAME="Zabbix server" \
      -e ZBX_SERVER_HOST="172.17.0.5" \
      --privileged -p 10050:10050 \
      -d zabbix/zabbix-agent:latest
```

```
1.停止当前主库
1.1 登录 10.80.16.63
1.2 执行 systemctl stop mysqld
1.3 检查进程 ps aux|grep mysql

2.打开备库
2.1 修改配置 vi /etc/my.cnf  
# 注释这条
# read_only=on
2.2 停止主备同步
mysql
mysql>stop slave;
mysql>stop slave status\G
2.3 打开读写模式
mysql>set global read_only=off;
mysql>show variables like '%read_only%';

3.后续操作，重新搭建备库
```

## zabbix api 文档

```
# zabbix aip文档
https://www.zabbix.com/documentation/6.2/en/manual/api
# 使用 pyzabbix 更简单，示例
https://github.com/lukecyca/pyzabbix/tree/master/examples
```

## venv环境迁移

```
# 将venv生成的虚拟环境迁移到新的电脑上，只需要修改 venv下的 pyvenv.cfg 文件中的python安装路径即可
# 同时可以修改active文件中的路径
```

‍

# 2023-03-08

```
# 通过linux代理访问windows3389   192.168.15.107是代理服务器，172.20.163.11是目标服务器
ssh -CfgN -L 3389:172.20.163.11:3389 192.168.15.107
```

# 2023-03-13

```
# 文件按大小排序 -S
ls -Shl
```

# 2023-03-14

```
# openjdk下载
https://developers.redhat.com/products/openjdk/download
```

## gradle项目打包配置

* 配置 build.gradle ，新增下面的内容，可以打出完整的包含依赖的jar包

```
jar {
    String someString = ''
    configurations.runtimeClasspath.each {
        someString = someString + "lib/" + it.name + " ";
    }
    manifest {
        attributes 'Main-Class': 'com.example.demo.DemoApplication'//这里填写入口函数所在类全限定名
        attributes 'Class-Path': someString
    }
}

//清除上次的编译过的文件
task clearPj(type: Delete) {
    delete 'build', 'target'
}
//把JAR复制到目标目录
task release(type: Copy, dependsOn: [build]) {
    from configurations.runtimeClasspath
    into 'build/libs/lib'
}
```

## annaconda下载安装

```
# 安装地址
https://repo.anaconda.com/archive/Anaconda3-2022.10-Windows-x86_64.exe
# 安装文档
https://docs.anaconda.com/anaconda/install/windows/
```

## jdbc链接oracle2种方式

```
# service
spring.datasource.url=jdbc:oracle:thin:@//192.168.15.106:1521/posp
## sid
#spring.datasource.url=jdbc:oracle:thin:@192.168.15.106:1521:posp
```

# 2023-03-15

```
1.读取外部文件可以使用 --加参数
2.可以默认读取同级目录下application.properties 或者同级目录下的config目录下的配置文件（优先级更高）
3.可以指定配置文件 --spring.config.location=e://xxx/xxx.properties
```

# 2023-03-16

```
conda env list
conda create -n py36 python=3.6
conda activate py36
```

# 2023-03-21

## tidb升级到v6.5.1版本

```
tar xzvf tidb-community-server-v6.5.1-linux-amd64.tar.gz
sh tidb-community-server-v6.5.1-linux-amd64/local_install.sh
source /home/tidb/.bash_profile

tar xf tidb-community-toolkit-v6.5.1-linux-amd64.tar.gz
ls -ld tidb-community-server-v6.5.1-linux-amd64 tidb-community-toolkit-v6.5.1-linux-amd64
cd tidb-community-server-v6.5.1-linux-amd64/
cp -rp keys ~/.tiup/
tiup mirror merge ../tidb-community-toolkit-v6.5.1-linux-amd64
tiup update cluster

tiup cluster check tidb-test --cluster 
# 不停机升级
tiup cluster upgrade tidb-test v6.5.1
# 停机升级
tiup cluster cluster stop tidb-test
tiup cluster upgrade tidb-test v6.5.1 --offline
tiup cluster start tidb-test

# 升级后验证
tiup cluster display tidb-test
```

## Fail    THP is enabled, please disable it for best performance

```
vi /etc/rc.d/rc.local
1:新增：
if test -f /sys/kernel/mm/transparent_hugepage/enabled; then
echo never > /sys/kernel/mm/transparent_hugepage/enabled
fi
if test -f /sys/kernel/mm/transparent_hugepage/defrag; then
echo never > /sys/kernel/mm/transparent_hugepage/defrag
fi
2:授权执行：
chmod +x /etc/rc.d/rc.local
3:重启：
reboot

vi /etc/sysctl.conf
net.core.somaxconn= 40000
net.ipv4.tcp_syncookies = 0
vm.swappiness = 0

vi /etc/security/limits.conf
soft nofile 1200000
hard nofile 1200000
tidb soft nofile 1000000
tidb hard nofile 1000000
tidb soft stack 32768
tidb hard stack 32768

vi /etc/pam.d/login
# 添加：
session required /lib/security/pam_limits.so
# 设置：
echo 1200000 > /proc/sys/fs/file-max

yum -y install numactl.x86_64
```

# 2023-03-22

```
# mysql 创建用户，修改密码
ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY 'tPNglviBAN8xKwDuehE1ng==';
flush privileges;
alter user user() IDENTIFIED WITH mysql_native_password BY 'tPNglviBAN8xKwDuehE1ng==';


create user 'yearing'@'%' IDENTIFIED WITH mysql_native_password BY 'tPNglviBAN8xKwDuehE1ng==';
grant all on *.* to 'yearing'@'%';
drop user 'yearing'@'%';
create user 'yearing'@'%' IDENTIFIED BY 'Yearing@123';
grant all on *.* to 'yearing'@'%';
flush privileges;
```

# 2023-03-23

```
# 文件过多时使用
find /log/profiles -type f  -mtime +2 |xargs rm -f
```

## centos7忘记开机密码,开机密码重置

```
# 启动界面按e进入，找到 Linux16  所在的行，在行末输入 init=/bin/sh
Ctrl+x 

mount -o remount,rw /
# 修改密码
passwd  

touch /.autorelabel

# 重启
exec /sbin/init
```

# 2023-03-24

```
# 用pyinstaller将django打包成exe文件，发现虽然可以运行，但是后台插件看不了，项目管理的后台也无法使用
settings.py
STATIC_ROOT = os.path.join('static', 'static_root')

urls.py
urlpatterns += static.static(settings.STATIC_URL, document_root=settings.STATIC_ROOT)

python manage.py collectstatic

# 生成配置文件 
pyi-makespec -D manage.py
pyinstaller manage.spec

manage.exe runserver --noreload
```

# 2023-03-27

```
# 批量杀掉某用户连接
mysql  -e 'show processlist'|grep 'debezium'|awk '{print "kill " $1 ";"}' >> k.sql
mysql < k.sql

# 按某列统计
mysql -e 'show processlist'|awk '{print $2}'|sort -r|uniq -c

# 重复则更新（需要有主键）
方法一：replace into 
方法二：
insert into dba_test.user values('1','hoyin','male',16)
on duplicate key  UPDATE sex=values(sex),age=values(age);
```

# 2023-03-28

## vscode配置自动却换到venv

```
首选项->设置（ctrl+,)
在搜索框中输入：python.venvPath
填入 venv
```

# 2023-03-29

```html
scrapy shell www.baidu.com -s USER_AGENT='Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/106.0.0.0 Mobile Safari/537.36' --nolog


//*[@class='ContentItem-title']//a/text()
```

# 2023-03-30

```html
ooceaceanbase安装
echo never > /sys/kernel/mm/transparent_hugepage/enabled

vi /etc/security/limits.conf
root soft nofile 655350
root hard nofile 655350
* soft nofile 655350
* hard nofile 655350
* soft stack 20480
* hard stack 20480
* soft nproc 655360
* hard nproc 655360
* soft core unlimited
* hard core unlimited

ulimit -a

vi /etc/sysctl.conf
# for oceanbase
## 修改内核异步 I/O 限制
fs.aio-max-nr=1048576

## 网络优化
net.core.somaxconn = 2048
net.core.netdev_max_backlog = 10000 
net.core.rmem_default = 16777216 
net.core.wmem_default = 16777216 
net.core.rmem_max = 16777216 
net.core.wmem_max = 16777216

net.ipv4.ip_local_port_range = 3500 65535 
net.ipv4.ip_forward = 0 
net.ipv4.conf.default.rp_filter = 1 
net.ipv4.conf.default.accept_source_route = 0 
net.ipv4.tcp_syncookies = 0 
net.ipv4.tcp_rmem = 4096 87380 16777216 
net.ipv4.tcp_wmem = 4096 65536 16777216 
net.ipv4.tcp_max_syn_backlog = 16384 
net.ipv4.tcp_fin_timeout = 15 
net.ipv4.tcp_max_syn_backlog = 16384 
net.ipv4.tcp_tw_reuse = 1 
net.ipv4.tcp_tw_recycle = 1 
net.ipv4.tcp_slow_start_after_idle=0

vm.swappiness = 0
vm.min_free_kbytes = 2097152

# 此处为 OceanBase 数据库的 data 目录
kernel.core_pattern = /data/core-%e-%p-%t

# 在线部署
yum install -y yum-utils
yum-config-manager --add-repo https://mirrors.aliyun.com/oceanbase/OceanBase.repo
yum install -y ob-deploy
source /etc/profile.d/obd.sh

# 离线部署
tar -xzf oceanbase-all-in-one-*.tar.gz
cd oceanbase-all-in-one/bin/
./install.sh
source ~/.oceanbase-all-in-one/bin/env.sh

obd web 
obd demo 
obd -h 
```

# 2023-03-31

```html
obd cluster deploy ob_dev -c local-example.yaml -A

[{"component":"oceanbase-ce","access_url":"127.0.0.1:2881","user":"root","password":"fdsafdsa","connect_url":"obclient -h127.0.0.1 -P2881 -uroot -p'fdsafdsa' -Doceanbase -A"},
{"component":"obproxy-ce","access_url":"127.0.0.1:2883","user":"root","password":"fdsafdsa","connect_url":"obclient -h127.0.0.1 -P2883 -uroot -p'fdsafdsa' -Doceanbase -A"},
{"component":"ocp-express","access_url":"192.168.15.109:8180","user":"admin","password":"oceanbase","connect_url":"http://192.168.15.109:8180"}]
obclient -h127.0.0.1 -P2881 -uroot -p'fdsafdsa' -Doceanbase -A
obclient -h127.0.0.1 -P2883 -uroot -p'fdsafdsa' -Doceanbase -A
http://192.168.15.109:8180  admin/Oce@n8aSe@1234
```

# 2023-04-03

```bash
# 安装docker
yum remove -y docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-selinux \
                  docker-engine-selinux \
                  docker-engine


yum install -y yum-utils device-mapper-persistent-data lvm2
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce
systemctl start docker
systemctl enable docker

vi setup_oms.sh
#!/bin/bash

# 配置
oms_meta_host=192.168.15.109
oms_meta_port=2883
oms_meta_user=root@oms
oms_meta_password="fsdR#fjkda#5325432FFFD43VVVV"
drc_rm_db=oms_rm
drc_cm_db=oms_cm
drc_cm_heartbeat_db=oms_cm_heartbeat
drc_user=drc
drc_password='Ffsaf#@3fdaDs#FFF1ffeaDDD34'
cm_url=http://192.168.15.111:8088
cm_location=100
cm_region=cn_beijing
cm_region_cn=北京
cm_is_default=true
cm_nodes=192.168.15.111
tsdb_url=192.168.15.220:8086
tsdb_username=root
tsdb_password=root

# 创建配置目录
mkdir -pv /root/oms/conf/

# 生成配置文件
echo '' > /root/oms/conf/config.yaml
echo "oms_meta_host: ${oms_meta_host}" >>/root/oms/conf/config.yaml
echo "oms_meta_port: ${oms_meta_port}" >>/root/oms/conf/config.yaml
echo "oms_meta_user: ${oms_meta_user}" >>/root/oms/conf/config.yaml
echo "oms_meta_password: ${oms_meta_password}" >>/root/oms/conf/config.yaml
echo "drc_rm_db: ${drc_rm_db}" >>/root/oms/conf/config.yaml
echo "drc_cm_db: ${drc_cm_db}" >>/root/oms/conf/config.yaml
echo "drc_cm_heartbeat_db: ${drc_cm_heartbeat_db}" >>/root/oms/conf/config.yaml
echo "drc_user: ${drc_user}" >>/root/oms/conf/config.yaml
echo "drc_password: '${drc_password}'" >>/root/oms/conf/config.yaml
echo "cm_url: ${cm_url}" >>/root/oms/conf/config.yaml
echo "cm_location: ${cm_location}" >>/root/oms/conf/config.yaml
echo "cm_region: ${cm_region}" >>/root/oms/conf/config.yaml
echo "cm_region_cn: ${cm_region_cn}" >>/root/oms/conf/config.yaml
echo "cm_is_default: ${cm_is_default}" >>/root/oms/conf/config.yaml
echo "cm_nodes:" >>/root/oms/conf/config.yaml
echo " - ${cm_nodes}" >>/root/oms/conf/config.yaml
echo "tsdb_service: 'INFLUXDB'" >>/root/oms/conf/config.yaml
echo "tsdb_enabled: true" >>/root/oms/conf/config.yaml
echo "tsdb_url: '${tsdb_url}'" >>/root/oms/conf/config.yaml
echo "tsdb_username: ${tsdb_username}" >>/root/oms/conf/config.yaml
echo "tsdb_password: ${tsdb_username}" >>/root/oms/conf/config.yaml


image=$(docker images|tail -1|awk '{print $3}')
# oceanbase oms部署
sudo docker run -d --name oms-config-tool ${image} bash && sudo docker cp oms-config-tool:/root/docker_remote_deploy.sh . && sudo docker rm -f oms-config-tool

sh docker_remote_deploy.sh -o /root/oms -c /root/oms/conf/config.yaml -i 192.168.15.111 -d ${image}


http://localhost:8090
http://localhost:8090/api/auth
{'taskParam': '{"password": "MjAyMy0wNC0wMw==", "user": "RM_oms"}'}
# 默认用户名和密码：admin/aaAA11__   密码改为：MjAyMy0wNC0wMw==
# 修改配置
# 根据业务需求，修改 config.yaml 文件。
# 执行命令 python -m omsflow.scripts.units.oms_init_manager --init-config-file。
# 执行命令 supervisorctl restart oms_console oms_drc_supervisor

ALTER SYSTEM SET system_memory = '50G' ZONE = 'zone1';
ALTER SYSTEM SET memory_limit_percentage = 80;

select svr_ip,id,zone,status from __all_server;

# oceanbase 集群启动
obd cluster start myoceanbase

show parameters like 'memory_limit';
```

# 2023-04-04

```bash
# 按照ip排序
mysql -e 'show processlist'|grep 'debezium'|awk '{print $3}'|cut -d: -f1|sort |uniq -c

# kill debezium进程
for i in `mysql -e 'show processlist'|grep 'debezium'|awk '{print $1}'`;do mysql -e "kill $i;";done

alter system set log_disk_size='30G';
show parameters like '%LOG_DISK_SIZE%';
ob_query_timeout 语句执行超时 默认：10秒
ob_trx_idle_timeout 事务空闲超时 默认：120秒
ob_trx_timeout  事务未提交超时 默认：100秒
ob_query_timeout 锁等待超时，推荐设置为10秒
set global ob_query_timeout = 3600000000;

analyze table ds_risk.bs_460_1 compute statistics for all columns size 128;

docker load -i oat_3.2.0_20220819_x86.tgz 
oat_image=`docker images | grep oat | awk '{printf $1":"$2"\n"}'`
docker run -d --net host --name oat -v /root/oat:/data --restart on-failure:5 $oat_image
```

# 2023-04-06

```bash
# percona的安装

yum install http://www.percona.com/downloads/percona-release/redhat/0.1-6/percona-release-0.1-6.noarch.rpm
yum install Percona-Server-server-57

# 下载percona
https://www.percona.com/downloads
# 需要下载的相关包
percona-icu-data-files-8.0.32-24.1.el7.x86_64.rpm  
percona-server-server-8.0.32-24.1.el7.x86_64.rpm  
percona-server-shared-compat-8.0.32-24.1.el7.x86_64.rpm
percona-server-client-8.0.32-24.1.el7.x86_64.rpm   
percona-server-shared-8.0.32-24.1.el7.x86_64.rpm  
percona-xtrabackup-80-8.0.32-26.1.el7.x86_64.rpm

grep pass /var/log/mysqld.log 
alter user user() identified by 'PassW0rd_';

# 配置复制用户
create user 'repl'@'192.168.%.%';
grant replication slave on *.* to 'repl'@'192.168.%.%';
ALTER USER 'repl'@'192.168.%.%' IDENTIFIED WITH mysql_native_password BY 'PassW0rd_';
flush privileges;

# /etc/my.cnf
# 从库配置 107 通道号
replicate-do-db=107:test5
server-id=7
slave-skip-errors=1062,1053,1146  #  跳过指定error no类型的错误
#slave-skip-errors=all #  跳过所有错误

# 多主一丛配置
# 主库配置需要同步的库
binlog-do-db = test5         #要同步的数据库

binlog-ignore-db = mysql     #不需要同步的数据库 
binlog_ignore_db = information_schema
binlog_ignore_db = performation_schema
binlog_ignore_db = sys


reset slave all;
stop slave ;

CHANGE MASTER TO
MASTER_HOST='192.168.15.107',
MASTER_PORT=3308,
MASTER_USER='repl',
MASTER_PASSWORD='PassW0rd_',
MASTER_LOG_FILE='mysql-bin.000225',
MASTER_LOG_POS=413979871
for channel '107';

CHANGE MASTER TO
MASTER_HOST='192.168.15.178',
MASTER_PORT=3306,
MASTER_USER='repl',
MASTER_PASSWORD='PassW0rd_',
MASTER_LOG_FILE='mysql-bin.000024',
MASTER_LOG_POS=8030463
for channel '178';

 stop slave for channel '178';
```

# 2023-04-10

```bash
# 存储信息更新
# 数据大屏
```

# 2023-04-11

```bash
# 查看已经建立的虚环境
conda env list
# 创建虚环境
conda create -n py36 python=3.6
# 激活虚环境
conda activate py36
# 在虚环境中安装包
conda install xxx   //安装xxx包

# tidb 收集表统计信息
ANALYZE TABLE order.orders_pos_pay_inf;
show analyze status where job_info = 'analyze columns';
show stats_histograms where table_name = 'orders_pos_pay_inf';
show stats_buckets;
# 看表统计信息监控状态
show stats_healthy where db_name='order' and table_name='orders_pos_pay_inf'; 

SHOW ANALYZE STATUS;
```

# 2023-04-12

```bash
# 制作启动盘
https://www.balena.io/etcher#download-etcher
```

# 2023-04-13

```bash
# grafana嵌入到其他页面
vi /usr/share/grafana/conf/defaults.ini
# 修改
allow_embedding = true  
cookie_samesite = disabled
serve_from_sub_path = true

# flink部署方式： 
#    Local——本地单机模式，学习测试使用 
#    Standlone——独立集群模式，flink自带集群，学习测试使用
#    standloneHA——独立集群的高可用模式，flink自带集群，开发测试使用
#    On Yarn——计算机资源同意由Hadoop Yarn管理，生产模式使用

https://blog.csdn.net/qq_24985201/article/details/122670276

curl -H "Authorization: Bearer eyJrIjoib2tSQ21JV2xQODV3bXRkbHlTUFlPQmdxM3luTlQxSlgiLCJuIjoiZGJhIiwiaWQiOjF9" http://10.80.3.26:30009/d/MQWgroii1/mysql-overview-1?orgId=1&refresh=5s

# nginx代理访问grafana
 upstream grafana {
        server 10.80.3.26:30009;
    }

    server {
        listen       80;
        server_name  localhost;

        #charset koi8-r;

        #access_log  logs/host.access.log  main;


        location / {
            proxy_buffer_size 128k;
            proxy_buffers 32 128k;
            proxy_busy_buffers_size 128k;

            add_header Access-Control-Allow-Origin '*';
            add_header Access-Control-Allow-Methods '*';
            add_header Access-Control-Allow-Credentials true;
            set $auth 'Bearer eyJrIjoib2tSQ21JV2xQODV3bXRkbHlTUFlPQmdxM3luTlQxSlgiLCJuIjoiZGJhIiwiaWQiOjF9';

            proxy_set_header HOST $host;
            proxy_set_header Authorization $auth;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_pass http://grafana/;
        }

# pycharm破解
--add-opens=java.base/jdk.internal.org.objectweb.asm=ALL-UNNAMED
--add-opens=java.base/jdk.internal.org.objectweb.asm.tree=ALL-UNNAMED

-javaagent:D:\soft\jetbra\ja-netfilter.jar=jetbrains
```

# 2023-04-14

```bash
# zabbix手册
https://www.zabbix.com/documentation/current/en/manual/api/reference/trigger/object#trigger
# 
```

# 2023-04-17

```bash
mysql floor函数 ，可以对小数取整
```

# 2023-04-18

```bash
# 火狐调出菜单栏
command + shift + f
```

# 2023-04-20

```bash
# 下载jdk
https://docs.aws.amazon.com/corretto/latest/corretto-11-ug/downloads-list.html

# mysql-cdc 下载地址
https://ververica.github.io/flink-cdc-connectors/release-2.3/content/connectors/mysql-cdc.html
```

# 2023-04-21

```bash
# maven阿里云仓库配置
<?xml version="1.0" encoding="UTF-8"?>
<settings xmlns="http://maven.apache.org/SETTINGS/1.2.0"
          xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
          xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.2.0 https://maven.apache.org/xsd/settings-1.2.0.xsd">

  <mirrors>
    <mirror>
        <id>nexus-aliyun</id>
        <mirrorOf>central</mirrorOf>
        <name>Nexus aliyun</name>
        <url>http://maven.aliyun.com/nexus/content/groups/public</url>
    </mirror>
  </mirrors>

</settings>

# 方式二：单项目配置
<project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd">
    <modelVersion>4.0.0</modelVersion>
    <groupId>com.test</groupId>
    <artifactId>conifg</artifactId>
    <packaging>war</packaging>
    <version>0.0.1-SNAPSHOT</version>

    <repositories>
        <repository>
            <id>central</id>
            <name>aliyun maven</name>
            <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
            <layout>default</layout>
            <!-- 是否开启发布版构件下载 -->
            <releases>
                <enabled>true</enabled>
            </releases>
            <!-- 是否开启快照版构件下载 -->
            <snapshots>
                <enabled>false</enabled>
            </snapshots>
        </repository>
    </repositories>

</project>

# maven
https://maven.apache.org/download.cgi

flink-13.5 
```

# 2023-04-23

```bash
# dm 忽略错误并恢复任务
tiup dmctl --master-addr 10.80.16.95:8261 query-status cloud_orders_trade_new 
tiup dmctl --master-addr 10.80.16.95:8261 handle-error cloud_orders_trade_new skip
tiup dmctl --master-addr 10.80.16.95:8261 resume-task cloud_orders_trade_new 

tiup dmctl --master-addr 10.80.17.19:8261 query-status cloud_order_20230420_1
```

# 2023-04-25

```bash
tiup cluster scale-out  tidb-cluster  scale_tiflash.yaml -uroot -p
tiup cluster display tidb-cluster
```

# 2023-04-27

```bash
 # tidb扩容，要一个一个扩

cat scale_tikv.yaml
tikv_servers:
  - host: 10.80.17.21
    port: 20161
    status_port: 20181
    deploy_dir: /data/tidb/kv/deploy/tikv-20161
    data_dir: /data/tidb/kv/data/tikv-20161
    log_dir: /data/tidb/kv/log/tikv-20161


tiup cluster check tidb-cluster scale_tikv.yaml --cluster --apply -uroot -p
tiup cluster scale-out  tidb-cluster  scale_tiflash.yaml -uroot -p

#  查询存储节点信息
select * from INFORMATION_SCHEMA.tikv_store_status

tiup ctl:v5.4.0 pd -u http://10.80.17.11:2379 -i
config show all
store
store weight 2364536 0.5 0.5
store remove-tombstone
config set leader-schedule-limit 4
config set region-schedule-limit 2048


# 将 ${version} 修改成实际需要的版本   tidb dm安装
tiup mirror clone tidb-dm-v6.5.1-linux-amd64 --os=linux --arch=amd64 \
    --dm-master=v6.5.1 --dm-worker=v6.5.1 --dmctl=v6.5.1 \
    --alertmanager=v0.17.0 --grafana=v6.5.1 --prometheus=v6.5.1 \
    --tiup=v$(tiup --version|grep 'tiup'|awk -F ' ' '{print $1}') --dm=v$(tiup --version|grep 'tiup'|awk -F ' ' '{print $1}')

#
tar czvf tidb-dm-v6.5.1-linux-amd64.tar.gz tidb-dm-v6.5.1-linux-amd64
```

# 2023-04-28

```bash
-- 查询正在执行的sql
select * from information_schema.processlist where command<>'Sleep'\G

-- 查看表统计信息是否准确
show stats_healthy where db_name='merchant' and table_name='marketing_policy_info'; 
-- 查看表tiflash是否开启 
select * from INFORMATION_SCHEMA.TIFLASH_REPLICA where table_name='orders_pos_pay_inf';
-- 开启tiflash
alter table merchant.marketing_rules_info set tiflash replica 1;

-- 统计信息收集
ANALYZE TABLE merchant.marketing_policy_info;

-- 
select count(*) from statistics.order_detail;
```

# 2023-05-04

```bash
cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governorcat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor
cpupower frequency-set -g performance
tiup cluster check tidb-cluster scale_tiflash.yaml --cluster --user tidb -p
tiup cluster scale-out  tidb-cluster  scale_tiflash.yaml -utidb -p
```

# 2023-05-06

```bash
# 可以读取pipfile来安装所有的包
pipenv install
# 查看安装的包
pipenv graph
# 查看所有虚拟环境
pipenv --venv
```

# 2023-05-08

```bash
# elasearch查询客户端
https://github.com/qax-os/ElasticHD/releases
# mysql同步数据到elasearch
go-mysql-es

# 用python 实现 订阅mysql binlog 同步到elasticsearch，要支持全量和增量同步，并且支持记录同步的日志点，发生错误，可以跳过日志点继续同步
```

# 2023-05-10

```bash
# tidb 缩容
tiup cluster scale-in  tidb-cluster -N 10.80.17.21:4000 
# tikv 缩容
tiup cluster scale-in  tidb-cluster -N 10.80.17.21:20160
```

# 2023-05-15

```bash
# redis统计key的数量
redis-cli info keyspace
oceanbase 社区版， 数据库v4.0.0版本，数据同步需使用OMS
商业版兼容Oracle，mysql
硬件最低要求2核8G，硬盘ssd54G以上，推荐4核16G以上
```

# 2023-05-17

```bash
# django 不在编辑页面中显示的字段
class JobAdmin(ModelAdmin):
    exclude = ('creator', 'created_date', 'modified_date')

    def save_model(self, request, obj, form, change):
        obj.creator = request.user
        super().save_model(request, obj, form, change)

# 默认创建时间
default=datetime.datetime.now
```

# 2023-05-18

```python
# 完善 MySQL to elasticsearch 同步程序，支持多表，记录pos点，读取ini配置文件功能
# admin.py 分组展示
fieldsets = (
    (None, {'fields':("userid","username",),(...)}), # 后面的(), 表示展示在一行
    ('第一轮面试记录',{'fields':("",)})
)

# 正则 =.*$  =后面的所有的字符
# $ 行尾替换
# pycharm join lines 将多行合并成一行
```

# 2023-05-19

```python
django management/commands/import_candidates.py 下实现命令行工具开发
python manage.py import_candidates --path file.csv
from django.core.management import BaseCommand
class Command():
    help =  '帮助信息'
    def add_arguments(self, parser):
        parser.add_argument('--path', type=str)

    def handle(self, *args, **kwargs):
        path = kwargs['path']
        ....

# openldap安装
docker run -p 389:389 -p 636:636 --name my-openldap-container --env LDAP_ORGANISATION="ihopeit" --env LDAP_DOMAIN="ihopeit.com" --env LDAP_ADMIN_PASSWORD="admin_passwd_4_ldap" --detach osixia/openldap:1.4.0

docker run -p 80:80 -p 443:443 --name phpldapadmin-service --hostname phpldapadmin-service --link my-openldap-container:ldap-host --env PHPLDAPADMIN_LDAP_HOSTS=ldap-host --detach osixia/phpldapadmin:0.9.0

LDAP_AUTH_URL = "ldap://101.200.59.94:389"
```

# 2023-05-22

```python
# django配置本地运行环境
# 新建包 settings 将settings.py拷贝到settings.py包下改名为base.py
# 新建local.py本地配置,覆盖base.py中的相关配置内容
# 修改mamage.py
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'settings.base')
# 指定本地设置
python manage.py runserver 0.0.0.0:8000 --settings=settings.local
```

# 2023-05-23

```python
pip install django-registration-redux
```

# 2023-05-26

```python
from django.utils.translation import gettext_lazy as _

class MyThing(models.Model):
    name = models.CharField(help_text=('This is the help text'))

django-admin makemessages -l zh_HANS -l en
django-admin compilemessages
```

# 2023-05-29

```python
What's goning on?
Did grandma die?
Uh,no,she's fine.
listen,Andrew
Sweetheart.it's been clear to us for sometime now that you've been unhappy.
All this acting out.
The pot smoking,the violence.
It just isn't you.
And we feel that you need to spend some quality time thinking about the goals in you life 
and what kind of person you want to be.
What the hell are you talking about?
There's this place called Camp Hennessey.
It's designed for teenagers like yourself who have lost their way and ...
you gotta be kiding.
They emphasize discipline and responsibility.
two things you're sorely in need of.
Hopefully,you won't have to be there for more than a few weeks.
You can stop this now.
I'm not going to any stupid camp for juvenile delinquents.
we've ready.
Sorry,honey,but you don't have a choice.
Son
get your hands off of me.Take it easy.
Andrew,Andrew,it's no use to fighting.
Now,these gentlemen are gonna help you get dressed
and then they're gonna drive you to the camp
We can take it from here. 
Wait,wait,Mom,mom,wait,please.
I'm sorry,mom,Please.
Honey,it's gonna be okay,I promise.
I know this was your idea.
Why you little... Stop it.
Honey,no matter what you say or do,I will always love you.
Let's go,come on.
```

# 2023-05-30

```python
MIDDLEWARE = [
'django.middleware.cache.UpdateCacheMiddleware',
'django.middleware.common.CommonMiddleware',
'django.middleware.cache.FetchFromCacheMiddleware',
]

# 配合检查
# tidb扩缩容
# 配合做79，63连接数优化
# 老系统数据下线
# Yearing（sql审核系统）升级
# mysql to es 数据同步工具
# 提数定时报表功能增加分页提取
# 配合投产和数据处理

tiup dmctl --master-addr 10.80.17.19:8261 handle-error  marketing_rebate_history_[0-9] skip
```

# 2023-06-02

```python
# inspectdb 根据现有数据库生成model
python manage.py inspectdb --database=running --settings=settings.local area city country provice > models.py

sandman2
```

# 2023-06-05

```python
# 优化按照行数拆分导出数据到excel的脚本
# 更新开发环境数据库服务器的账号密码
# 配合 江苏备捡 数据处理
```

# 2023-06-06

```python
# 配合 江苏人行检查 agent.agent_trade_addr 表导数
# 工单审核系统 增加交易上送时间
```

# 2023-06-07

```python
# openldap搭建
docker pull osixia/openldap
docker pull osixia/phpldapadmin
mkdir -p /usr/local/ldap && cd /usr/local/ldap
docker run -d -p 389:389 -p 636:636 --name my-openldap-container --env LDAP_ORGANISATION="ihopeit" --env LDAP_DOMAIN="ihopeit.com" --env LDAP_ADMIN_PASSWORD="admin_passwd_4_ldap" --detach osixia/openldap:1.4.0
docker run -p 80:80 -p 443:443 --name phpldapadmin-service --hostname phpldapadmin-service --link my-openldap-container:ldap-host --env PHPLDAPADMIN_LDAP_HOSTS=ldap-host --detach osixia/phpldapadmin:0.9.0
# 登录用户名：
cn=admin,dc=ihopeit,dc=com
# 登录密码：  
admin_passwd_4_ldap

# docker环境的安装
sudo yum install -y yum-utils
sudo yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo

安装docker-ce，containerd和docker-compose，如
sudo yum install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

yum install epel-release -y
yum install -y yum-utils
yum install centos-release-scl -y
yum install rh-python38 -y
scl enable rh-python38 bash
yum install rh-python38-python-devel -y
```

# 2023-06-08

```python
pip install django-python3-ldap
# dm同步不支持修改主键，修改主键会导致同步暂停
# 开发环境编写脚本定时清理tiflash崩溃导致的core文件占满存储空间问题
```

# 2023-06-09

```python
# 开发环境oracle开发库故障恢复
```

# 2023-06-12

```python
# 黄兆臣sql性能分析和优化
# 在使用order by 和limit情况下，order by的条件在where条件中也应该存在，否则会导致执行计划不是最优。
# 用top 10 来进行结果集测试

# tidb 统计信息  表状态，统计信息收集，索引优化 
show stats_healthy where db_name='statistics' and table_name='order_detail'; 
```

# 2023-06-13

```python
# Jupyter 魔法命令
%run hello.py
%timeit  %time
%%timeit %%time
lsmagic

# 编写统计信息收集定时任务脚本
# tidb中部份表做优化
```

# 2023-06-15

```python
# cloud 备库宕机，更换一台主机搭建备库 10.80.16.104
# 
# percona8安装
tar xvf Percona-Server-8.0.30-22-r7e301439b65-el7-x86_64-bundle.tar
yum localinstall percona-server-*.rpm percona-icu-data-files-8.0.30-22.1.el7.x86_64.rpm -y

systemctl start mysqld
systemctl enable mysqld
systemctl status mysqld

# 修改密码
grep 'temp' /var/log/mysqld.log 
alter user user() identified by 'hqZCj3n&S*gzxm4f';
# 安装 libev-4.15-7.el7.x86_64.rpm
# 下载地址 mirror.centos.org/centos/7/extras/x86_64/Packages/libev-4.15-7.el7.x86_64.rpm
# 安装xtrabackup
tar xvf Percona-XtraBackup-8.0.30-23-r873b467185c-el7-x86_64-bundle.tar
yum localinstall percona-xtrabackup-80-8.0.30-23.1.el7.x86_64.rpm
# 安装qpress
yum localinstall qpress-11-3.el7.x86_64.rpm
# 原备库修复测试
```

# 2023-06-16

```python
# 测试环境 15.195 宕机修复
# orders_pos_pay_inf 表性能问题分析
# 优化TiDB每月定时收集统计信息任务脚本，增加日志功能
```

# 2023-06-19

```
# 搬家，工位整理
```

# 2023-06-20

```
# 数据备份使用压缩格式备份
# 数据迁移，腾退办公电脑
# 创建虚环境
conda create -n cmdb_env python=3.9
conda activate cmdb_env
# 整理备份文档
```

# 2023-06-21

```shell
# pet库恢复数据  
mysqldump -uroot -pabc@123 -S /home/mysql/mysql-files/mysql_3306.sock --set-gtid-purged=OFF   xxl_job > xxl_job.sql
# 导入空库使用 -d


# 让securecrt标签页显示session中的名称  
# session options -> Terminal-> Advanced
# 勾选 Ignore window title change requests
# yinhb hqZCj3n&S*gzxm4f

#  查询日志中是否存在删库
mysqlbinlog mysql-bin.000355 --base64-output=decode-rows -vv --skip-gtids=true |grep -C 3 -i 'drop database' 
 mysql-bin.000355 --base64-output=decode-rows -vv --skip-gtids=true > 1.log

# 取消权限
revoke drop on *.*  from canal;
```

# 2023-06-25

```
# 数据下线脚本批量生成工具编写
# 上半年工作总结
1.TIDB优化：增加21节点服务器，扩展tidb服务， tidb升级测试v6.5.1版本。新增大表统计信息收集定时任务。新增清理tiflash core dump文件脚本。TiDB部分表健康状态检查，索引优化,对部分未使用和重复索引进行删除。
2.日常邮件处理：处理数据脚本和数据提取邮件总共大于470封。
3.配合完成江苏人行检查。
4.数据库备份脚本优化。增加压缩参数，减小MySQL数据库备份体积到小于原来的1/4。
5.数据大屏Dataease维护。优化数据收集后台程序性能。统计收集和监控存储使用情况。告警信息综合展示。对网络，系统，数据库中的重要监控指标集中展示。
6.新增cloud行业库slave从库（10.80.16.104）一台，以解决原slave（10.80.16.97）硬件故障。
7.下线BI数据库服务器（10.80.16.62），资源回收。
8.报表系统升级，采用XXL_JOB发送定时任务，维护报表49个。
9.开发库维护，新增两套（192.168.16.142，143）Cloud环境MySQL数据库。
10.Yearing开发库使用，SQL审核，系统升级。创建维护账号42个，审核工单162个。
11.更新开发环境数据库服务器的账号密码。
12.搭建基于Django的信息管理系统，整理历史下线数据存储清单并录入系统。
13.优化数据导出脚本，新增按照行数拆分导出数据到excel。
14.数据下线脚本批量生成工具编写，整理下一批次下线数据脚本。
15.Oceanbase搭建，测试及功能探索。
16.开发mysql to elasticsearch数据同步工具。
17.成都灾备机房筹备，带宽测试，灾备主从同步性能测试。
```

# 2023-06-25

```
# 投产
# 账务库数据下线
# 行业pet库 ew_order_his 数据同步问题  192.168.15.221->192.168.15.208
# wallet_order.ew_order_his_221_208 源表分表结构不一致导致无法创建同步任务，但也没有报错信息，可能是dm存在bug导致
```

# 2023-06-28

```
# http://192.168.15.218:8261/dashboard/
排查解决dm任务由于binlog过期导致pause
排查解决任务名称过长，过长会导致pause
优化邮件统计程序，使用并发执行
192.168.15.107 mysql宕机修复,由于mysql与oracle争抢资源导致，计划迁移mysql
```

# 2023-06-29

```bash
# pycharm 破解
-javaagent:/Users/mac/Documents/ja-netfilter/ja-netfilter.jar
--add-opens=java.base/jdk.internal.org.objectweb.asm=ALL-UNNAMED
--add-opens=java.base/jdk.internal.org.objectweb.asm.tree=ALL-UNNAMED

# centos mysql 5.7.32 安装

# 二进制安装
1 下载地址
https://dev.mysql.com/downloads/mysql/
https://cdn.mysql.com//Downloads/MySQL-5.7/mysql-5.7.21-linux-glibc2.12-x86_64.tar.gz

2 创建mysql账号
groupadd -g 27 mysql
useradd -u 27 -r -g mysql -s /bin/false mysql

3 准备目录
mkdir /home/mysql/{data,tmp,conf,logs} -pv
chown -R mysql.mysql /home/mysql

4 准备安装文件
tar zvxf mysql-5.7.*.tar.gz -C /usr/local/
ln -sv /usr/local/mysql-5.7.* /usr/local/mysql

5 准备配置文件
vi /home/mysql/conf/my.cnf
[mysqld]
user=mysql
port=3306
basedir=/usr/local/mysql
pid-file=/home/mysql/data/mysql.pid
datadir=/home/mysql/data
tmpdir=/home/mysql/tmp
log-error=/home/mysql/logs/error.log


# 字符集设置
character-set-server = utf8mb4
collation-server = utf8mb4_unicode_ci
init_connect='SET NAMES utf8mb4'

# 开启 LOAD LOCAL INFILE,从文件导入数据
local_infile = 1

# 默认数据库时区
default-time-zone='+8:00'

# 日志显示默认时区
explicit_defaults_for_timestamp

# 包大小，连接数，文件数限制
max_allowed_packet = 512M
max_connections = 10240
max-connect-errors=100000
open_files_limit = 65535
innodb_open_files=65535

#ONLY_FULL_GROUP_BY  SQL模式
sql_mode ='STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION'
# server-id=73

# gtid模式
# gtid_mode = on
# enforce-gtid-consistency = true

binlog_format = row
log-bin = mysql-bin



6.复制链接到/etc/my.cnf
mv /etc/my.cnf /etc/my.cnf.org
ln -sv /home/mysql/conf/my.cnf /etc/my.cnf

7.初始化
/usr/local/mysql/bin/mysqld --defaults-file=/etc/my.cnf  --initialize 

grep 'temporary password' /home/mysql/logs/error.log

8.启动
/usr/local/mysql/bin/mysqld_safe &

# 配置PATH
echo "export PATH=$PATH:/usr/local/mysql/bin" >> /etc/profile
source /etc/profile


9.建用户,改密码
mysql -uroot -p
set password for 'root'@'localhost'='PassW0rd_';
create user test;
create user 'test'@'%' identified by 'test';
flush privileges;


10.创建~/.my.cnf
vi ~/.my.cnf
[mysql]
user=root
password=PassW0rd_


# centos7配置系统服务
vi /usr/lib/systemd/system/mysqld.service
[Unit]
Description=mysql
After=network.target remote-fs.target nss-lookup.target

[Service]
Type=forking
ExecStart=/usr/local/mysql/support-files/mysql.server start
#ExecReload=/usr/local/mysql/support-files/mysql.server restart
#ExecStop=/usr/local/mysql/support-files/mysql.server stop
#PrivateTmp=true

[Install]
WantedBy=multi-user.target
Documentation=man:mysqld
After=network.target
After=syslog.target
WantedBy=multi-user.target
User=mysql
Group=mysql
PIDFile=/home/mysql/data/mysql.pid
TimeoutSec=0
PermissionsStartOnly=true
ExecStart=/usr/local/mysql/bin/mysqld --daemonize --pid-file=/home/mysql/data/mysql.pid

yum -y install psmisc
killall mysqld
systemctl daemon-reload
systemctl start mysqld
systemctl status mysqld

# 关闭
mysqladmin --defaults-file=/data1/mysql_3307/my.cnf -uroot -p -S /data1/mysql_3307/mysql.sock shutdown

# 安装percona
# https://docs.percona.com/percona-xtrabackup/2.4/installation/yum_repo.html#installing-percona-xtrabackup-from-percona-yum-repository

# xtrabackup安装
https://repo.percona.com/yum/release/7Server/RPMS/x86_64/
wget https://repo.percona.com/yum/release/7Server/RPMS/x86_64/percona-xtrabackup-24-2.4.9-1.el7.x86_64.rpm

# 安装qpress
https://pkgs.org/download/qpress
yum install https://repo.percona.com/yum/release/7/RPMS/x86_64/qpress-11-3.el7.x86_64.rpm

# 查看系统ulimit
ulimit -n
# ulimit -n 40960 # 当即生效
cat /etc/security/limits.conf
* soft nofile  1000000
* hard nofile  1000000
* soft nproc  1000000
* hard nproc  1000000
cat /etc/security/limits.d/20-nproc.conf 
*          soft    nproc     1000000
root       soft    nproc     unlimited
*  -  fsize    unlimited
*  -  cpu     unlimited
*  -  as      unlimited
*  -  nofile   1000000
*  -  nproc   1000000

cat /etc/sysctl.conf
fs.file-max = 1000000

# 查看mysql的socket位置
mysql_config --socket

# 压缩备份
xtrabackup --user=root --password="PassW0rd_" --host=localhost --socket=/tmp/mysql.sock --port=3308 --backup  --stream=xbstream --compress --parallel=4  --target-dir=/home/mysql/backup/data/full_`date +%Y_%m_%d_%H-%M`/ > /home/mysql/backup/data/full_`date +%Y_%m_%d_%H-%M`.backup.xbstream 



ambari        | %           |
| bi            | %           |
| canal         | %           |
| data          | %           |
| dba           | %           |
| jumpserver    | %           |
| maxwell       | %           |
| nacos_paas    | %           |
| nacos_saas    | %           |
| paas          | %           |
| root          | %           |
| sonar         | %           |
| telecomfraud  | %           |
| wallet        | %           |
| wjw           | %           |
```

# 2023-06-30

```
# 整理归档备份文件
# 文件批量改名  ${i%?}  删除最后一个字符
# 例如将 bkp_interface_call_his_2022_9.sql.gz? 改成 bkp_interface_call_his_2022_9.sql.gz
# 删除最后2字符 ${a%??}
for i in `find . -name '*gz?'`;do mv $i ${i%?};done

# 迁移cmdb到提数服务器
# 协助使用脚本处理数据，优化存储空间
```

# 2023-07-03

```
# pipenv --venv  查看本地的虚环境
vi ~/.zshrc
export PIPENV_VENV_IN_PROJECT=1
# 随便安装一个包就会在当前.venv 下创建虚环境，并且在你进入的时候自动激活这个虚拟环境，之后按照正常pip install 安装包即可
pipenv --python 3.9 install cchardet

# 优化邮件统计脚本，增加并行收取邮件
```

# 2023-07-04

```
# 数据下线，执行drop脚本
```

# 2023-07-06

```
# 迁移部署email邮件统计程序到192.168.15.195上
# centos7 安装python3.9
https://blog.csdn.net/weixin_71435518/article/details/131267114
wget https://www.python.org/ftp/python/3.9.0/Python-3.9.0.tgz
tar -zxvf Python-3.9.0.tgz
yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc make
mkdir -p /usr/local/python3
cd Python-3.9.0
./configure --prefix=/usr/local/python3
make && make install
ln -s /usr/local/python3/bin/python3.9 /usr/bin/python3
ln -s /usr/local/python3/bin/pip3.9 /usr/bin/pip3
vi ~/.bash_profile
export PYTHON_HOME=/usr/local/python3
export PATH=$PYTHON_HOME/bin:$PATH 

# 迁移cloud的ds_order库表到pet环境
```

# 2023-07-10

```
# 修复paas恢复脚本
# 清理tidb03日志，解决空间告警问题
# 和朱凯威讨论mysql无主键完全一样数据如何去重问题。
-- 假设您的表名为 `your_table`
-- 先为表添加一个临时的唯一 ID 列
ALTER TABLE your_table ADD temp_id INT UNSIGNED AUTO_INCREMENT PRIMARY KEY;

-- 执行删除操作，只保留具有最小 temp_id 值的重复记录
DELETE t1
FROM your_table t1
JOIN your_table t2
ON t1.column1 = t2.column1
AND t1.column2 = t2.column2
-- 添加更多列进行比较，以确保记录完全相同
AND t1.temp_id > t2.temp_id;
```

# 2023-07-11

```
# 清理clickhouse，腾出tidb03根分区空间
# 行业dm 任务cloud_common pause 修复
tiup dmctl --master-addr 10.80.16.95:8261 handle-error cloud_common skip  #只能跳过ddl，dml不支持
# 查看binlog执行的内容
show binlog events in 'binlog.000599' from 584305541;
```

# 2023-07-12

```
# 清理临时表 加_t后缀，保留一周，一周后清理
rename table ds_org_merchant.bj_20210420 to ds_org_merchant.bj_20210420_t; 
rename table ds_org_merchant.bkp_org_open_prod_0321 to ds_org_merchant.bkp_org_open_prod_0321_t; 
rename table ds_org_merchant.bkp_org_promo_code_0321 to ds_org_merchant.bkp_org_promo_code_0321_t; 
rename table ds_org_merchant.cus_mer_excel_bak to ds_org_merchant.cus_mer_excel_bak_t; 
rename table ds_org_merchant.cus_mer_excel_del_temp to ds_org_merchant.cus_mer_excel_del_temp_t; 
rename table ds_org_merchant.customer-api-79f6787697-7tx75.log.2021-07-09.5 to ds_org_merchant.customer-api-79f6787697-7tx75.log.2021-07-09.5_t; 
rename table ds_org_merchant.dabao_temp to ds_org_merchant.dabao_temp_t; 
rename table ds_org_merchant.hashdata to ds_org_merchant.hashdata_t; 
rename table ds_org_merchant.ide_info_tmp_0410 to ds_org_merchant.ide_info_tmp_0410_t; 
rename table ds_org_merchant.ide_info_tmp_0413 to ds_org_merchant.ide_info_tmp_0413_t; 
rename table ds_org_merchant.mer_20220929 to ds_org_merchant.mer_20220929_t; 
rename table ds_org_merchant.mer_extend_info_baktime to ds_org_merchant.mer_extend_info_baktime_t; 
rename table ds_org_merchant.mer_info_259_old to ds_org_merchant.mer_info_259_old_t; 
rename table ds_org_merchant.mer_info_baktime to ds_org_merchant.mer_info_baktime_t; 
rename table ds_org_merchant.mer_ls_0412 to ds_org_merchant.mer_ls_0412_t; 
rename table ds_org_merchant.mer_ls_0414 to ds_org_merchant.mer_ls_0414_t; 
rename table ds_org_merchant.mer_ls_0421 to ds_org_merchant.mer_ls_0421_t; 
rename table ds_org_merchant.mer_ls_0422 to ds_org_merchant.mer_ls_0422_t; 
rename table ds_org_merchant.mer_ls_0423 to ds_org_merchant.mer_ls_0423_t; 
rename table ds_org_merchant.mer_risk_20210323 to ds_org_merchant.mer_risk_20210323_t; 
rename table ds_org_merchant.mer_stock_info to ds_org_merchant.mer_stock_info_t; 
rename table ds_org_merchant.org_info_fer_join to ds_org_merchant.org_info_fer_join_t; 
rename table ds_org_merchant.org_info_t to ds_org_merchant.org_info_t_t; 
rename table ds_org_merchant.org_open_account_20210511 to ds_org_merchant.org_open_account_20210511_t; 
rename table ds_org_merchant.route_ide_tmp_20201102 to ds_org_merchant.route_ide_tmp_20201102_t; 
rename table ds_org_merchant.set_pay_order_info_0827 to ds_org_merchant.set_pay_order_info_0827_t; 
rename table ds_org_merchant.set_pay_order_info_0827_test to ds_org_merchant.set_pay_order_info_0827_test_t; 
rename table ds_org_merchant.sn_tmp_20211202 to ds_org_merchant.sn_tmp_20211202_t; 
rename table ds_org_merchant.t_merchant_trade to ds_org_merchant.t_merchant_trade_t; 
rename table ds_org_merchant.temp2 to ds_org_merchant.temp2_t; 
rename table ds_org_merchant.temp_lisj to ds_org_merchant.temp_lisj_t; 
rename table ds_org_merchant.temp_lt to ds_org_merchant.temp_lt_t; 
rename table ds_org_merchant.temp_marketing_rule_element to ds_org_merchant.temp_marketing_rule_element_t; 
rename table ds_org_merchant.temp_sn to ds_org_merchant.temp_sn_t; 
rename table ds_org_merchant.temp_union_merchant to ds_org_merchant.temp_union_merchant_t; 
rename table ds_org_merchant.terminal_ide_installation_temp to ds_org_merchant.terminal_ide_installation_temp_t; 
rename table ds_org_merchant.terminal_key_001 to ds_org_merchant.terminal_key_001_t; 
rename table ds_org_merchant.terminal_temp to ds_org_merchant.terminal_temp_t; 
rename table ds_org_merchant.terminal_transfer_detail_bak to ds_org_merchant.terminal_transfer_detail_bak_t; 
rename table ds_org_merchant.terminal_transfer_detail_copy1 to ds_org_merchant.terminal_transfer_detail_copy1_t; 
rename table ds_org_merchant.test_20210621 to ds_org_merchant.test_20210621_t; 
rename table ds_org_merchant.test_hxn to ds_org_merchant.test_hxn_t; 
rename table ds_org_merchant.test_hzc_2022_1_21 to ds_org_merchant.test_hzc_2022_1_21_t; 
rename table ds_org_merchant.test_hzc_2022_1_21_area to ds_org_merchant.test_hzc_2022_1_21_area_t; 
rename table ds_org_merchant.tmp_mer_orders_netpay_total_20210919 to ds_org_merchant.tmp_mer_orders_netpay_total_20210919_t; 
rename table ds_org_merchant.tmp_ym_20220218 to ds_org_merchant.tmp_ym_20220218_t; 

#  优化备份脚本，使用xbstream，63上先做测试
# centos7 安装xtrabackup
yum localinstall percona-xtrabackup-24-2.4.18-1.el7.x86_64.rpm 
Error: Package: percona-xtrabackup-24-2.4.18-1.el7.x86_64 (/percona-xtrabackup-24-2.4.18-1.el7.x86_64)
           Requires: libev.so.4()(64bit)
yum install libev  -y 
yum install yum-plugin-downloadonly
yum install --downloadonly --downloaddir=./ libev

/bin/innobackupex --backup  --slave-info --safe-slave-backup --user=root --password=root --stream=xbstream --compress -S /home/mysql/data/mysql.sock --no-timestamp /home/mysql/backup/data/full_`date +%Y_%m_%d_%H-%M` > /home/mysql/backup/data/full_`date +%Y_%m_%d_%H-%M`.backup.xbstream 2>/tmp/full_`date +%Y_%m_%d_%H-%M`.log
```

# 2023-07-13

```
# 下线 ds_account_1-12.bkp_frozen_info_2020_9-12
```

# 2023-07-16

```
开发测试机房迁移到昌平机房
Error: failed to fetch cpu-arch or kernel-name: executor.ssh.execute_failed: Failed to execute command over SSH for 'root@192.168.15.225:22' {ssh_stderr: , ssh_stdout: , ssh_command: export LANG=C; PATH=$PATH:/bin:/sbin:/usr/bin:/usr/sbin uname -m}, cause: ssh: handshake failed: ssh: unable to authenticate, attempted methods [none password], no supported methods remain

# root密码不对导致的

pvcreate /dev/sdb
vgcreate myvg /dev/sdb
lvcreate -n mylv -l 100%FREE myvg
mkfs.xfs /dev/myvg/mylv
vi /etc/fstab
/dev/myvg/mylv          /home           xfs     defaults        0       0

mount -a

# docker 安装
yum remove -y docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-selinux \
                  docker-engine-selinux \
                  docker-engine


yum install -y yum-utils device-mapper-persistent-data lvm2

yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce
systemctl start docker
systemctl enable docker

# docker安装clickhouse
docker pull yandex/clickhouse-server
docker pull yandex/clickhouse-client
docker run -d --name clickhouse-dev-server --ulimit nofile=262144:262144 \
-p9000:9000 -p 9009:9009 yandex/clickhouse-server

mkdir -pv /home/clickhlouse/{clickhouse-dev-db,config,log}

docker cp clickhouse-dev-server:/etc/clickhouse-server/config.xml /home/clickhlouse/config
docker cp clickhouse-dev-server:/etc/clickhouse-server/users.xml /home/clickhlouse/config
echo "123456";echo -n "123456"|sha256sum|tr -d '-'

docker run -d --name clickhouse-dev-server --ulimit nofile=262144:262144 \
-v /home/clickhlouse/clickhouse-dev-db:/var/lib/clickhouse -p8123:8123 \
-v /home/clickhlouse/log:/var/log/clickhouse-server:rw \
-v /home/clickhlouse/config/config.xml:/etc/clickhouse-server/config.xml \
-v /home/clickhlouse/config/users.xml:/etc/clickhouse-server/users.xml \
-p9000:9000 -p 9009:9009 yandex/clickhouse-server

docker run -it --rm --link clickhouse-dev-server:clickhouse-dev-server yandex/clickhouse-client --host clickhouse-dev-server

tiup dm scale-in dm-pet -N 192.168.15.237:8262 --force
tiup dm scale-in dm-pet -N 192.168.15.225:8263 --force
tiup dm scale-in dm-pet -N 192.168.15.225:8266 --force
tiup dm scale-in dm-pet -N 192.168.15.225:8264 --force
192.168.15.242:9000

create database petcloudnacos;
create database petoceannacos;
create database petwalletnacos;

create database testcloudnacos;
create database testoceannacos;
create database testwalletnacos;

create user 'test_nacos'@'%' IDENTIFIED WITH mysql_native_password BY 'tPNglviBA!N8';
grant all on testcloudnacos.* to 'test_nacos'@'%';
grant all on testoceannacos.* to 'test_nacos'@'%';
grant all on testwalletnacos.* to 'test_nacos'@'%';

create user 'pet_nacos'@'%' IDENTIFIED WITH mysql_native_password BY 'Dew23f@Fa';
grant all on petcloudnacos.* to 'pet_nacos'@'%';
grant all on petoceannacos.* to 'pet_nacos'@'%';
grant all on petwalletnacos.* to 'pet_nacos'@'%';
```

# 2023-07-17

```
# 研究 tidb dm-server 迁移方法
# 迁移开发库192.168.15.213-cloud 192.168.15.225 到更稳定的服务器上
# 检查压缩备份
# 82 订单库 ds_cis 体积太大，优化方案

test_nacos   tPNglviBA!N8
pet_nacos  Dew23f@Fa
142,134合并
tidb合并

Host '192.168.16.140' is blocked because of many connection errors; unblock with 'mysqladmin flush-hosts'
mysql> show variables like '%max_connnect_errors%';
Empty set (0.00 sec)

mysql> show variables like '%errors%';
+---------------------+-------+
| Variable_name       | Value |
+---------------------+-------+
| log_query_errors    |       |
| max_connect_errors  | 100   |
| replica_skip_errors | OFF   |
| slave_skip_errors   | OFF   |
+---------------------+-------+
4 rows in set (0.01 sec)

mysql> set global max_connect_errors=1000000;
Query OK, 0 rows affected (0.00 sec)

mysql> show variables like '%errors%';
+---------------------+---------+
| Variable_name       | Value   |
+---------------------+---------+
| log_query_errors    |         |
| max_connect_errors  | 1000000 |
| replica_skip_errors | OFF     |
| slave_skip_errors   | OFF     |
+---------------------+---------+
4 rows in set (0.00 sec)

mysql> flush hosts;
Query OK, 0 rows affected, 1 warning (0.00 sec)
```

# 2023-07-18

```
# 搭建postgresql测试服务器
# centos7 root忘记密码
在 GRUB 启动菜单中选择 CentOS 7，并按下 e 键进入编辑模式。
在编辑模式中，找到以 linux16 开头的行，并在行尾添加 rd.break 参数。
例如，你可能会将其修改为 linux16 /vmlinuz-3.10.0-123.el7.x86_64 root=/dev/mapper/centos-root ro 
ro改成 rw init=/sysroot/bin/sh
按下 Ctrl + x 或 F10 键启动 CentOS 7。
mount -o remount,rw /sysroot
chroot /sysroot
passwd root
touch /.autorelabel
exit
mount -o remount,ro /sysroot
reboot

# postgresql
https://www.postgresql.org/download/linux/redhat/
sudo yum install -y https://download.postgresql.org/pub/repos/yum/reporpms/EL-7-x86_64/pgdg-redhat-repo-latest.noarch.rpm
yum install epel-release.noarch -y
yum install libzstd.x86_64 -y
yum install -y postgresql15-server
/usr/pgsql-15/bin/postgresql-15-setup initdb
systemctl enable postgresql-15
systemctl start postgresql-15

# yum install postgresql-server
# postgresql-setup --initdb
# systemctl enable postgresql.service
# systemctl start postgresql.service

# 配置远程连接
vi /var/lib/pgsql/15/data/postgresql.conf
listen_addresses = '*'
vi /var/lib/pgsql/15/data/pg_hba.conf
host    all             all             0.0.0.0/0       scram-sha-256

systemctl restart postgresql-15

ALTER USER postgres WITH PASSWORD 'postgres';

# 创建逻辑卷
pvcreate /dev/sdc
vgcreate data02 /dev/sdc
lvcreate -n data -l 100%FREE data02
mkfs.xfs /dev/data02/data
# mount /dev/data02/data /home/mysql

echo "/dev/data02/data /home/mysql xfs defaults 0 0" >> /etc/fstab
mount -a

# 修改sshd配置，允许root登录
cp /etc/ssh/sshd_config /etc/ssh/sshd_config.bak
sed -i 's/PermitRootLogin no/PermitRootLogin yes/g' /etc/ssh/sshd_config
service sshd restart

# sftp
# 登录
sftp 10.33.2.1
# 转换目录
cd/lcd   # 远程/本地
ls/lls  # 远程/本地
# put -a 断点续传
put -a /home/mysql/backup/paas_2023_07_18_01-00.backup.xbstream
```

# 2023-07-19

```
# 拷贝数据到成都灾备机房
create user 'root'@'10.33.2.%' identified by 'F98dbSfaWZC8rpz8';
grant all on *.* to 'root'@'10.33.2.%';

# 开发库192.168.16.125 dm-worker-1 扩容逻辑卷
```

# 2023-07-21

```
# 成都灾备库paas库同步，清结算qjs库数据传输
# 完成开发库tidb迁移
# 核对下线数据
```

# 2023-07-24

```
# 灾备库数据同步
http://192.168.15.225:8261/dashboard/migration/task    dm worker 显示 offline，重启自动恢复
```

# 2023-07-25

```
# 清理dba_monitor.processlist运维数据
# 回收开发服务器192.168.15.107资源

create tablespace tbs_DZQZ datafile '/data/posp/tbs_DZQZ_01.dbf' size 5G autoextend on next 1000M maxsize unlimited;
CREATE USER DZQZ IDENTIFIED BY DZQZ DEFAULT TABLESPACE tbs_DZQZ;
GRANT CONNECT, RESOURCE TO DZQZ;


impdp  \'/ as sysdba \'directory=DPDATA dumpfile=expdat01.dmp logfile=expdat01.log 
```

# 2023-07-26

```
# oceanbase 搭建，与mysql兼容性
# 数据同步情况监测
# 查看dm-worker中的错误信息 
grep -A 6 'ERROR' dm-worker.log |more

vi /etc/security/limits.conf
* soft nofile  665535
* hard nofile  1000000
* soft nproc  65535
* hard nproc  65535

# 改完需要重启

# https://www.oceanbase.com/docs/common-oceanbase-database-10000000001692850
tar -xzf oceanbase-all-in-one-*.tar.gz
cd oceanbase-all-in-one/bin/
./install.sh
source ~/.oceanbase-all-in-one/bin/env.sh
obd demo --home_path=/home/oceanbase
obd cluster list
obd cluster display demo
obd cluster reload demo
obd cluster restart demo -c obproxy-ce --wp
obd cluster stop demo -s server1
obd cluster destroy demo -f

# 登录，root 密码是 空
obclient -h127.0.0.1 -P2881 -uroot@sys -Doceanbase -A

# 创建租户 obmysql     集群名称是demo 默认用户是root，密码是空
obd cluster tenant create demo -n obmysql --max-cpu=4 --memory-size=2G --log-disk-size=3G --max-iops=10000 --iops-weight=2 --unit-num=1 --charset=utf8mb4
# 查看租户 
obd cluster tenant show demo
# 登录
obclient -h127.0.0.1 -P2881 -uroot@obmysql -Doceanbase -A
# 删除租户
obd cluster tenant drop demo -n obmysql

# 编辑集群配置，比如修改 memory_limit
obd cluster edit-config demo
# 修改完重新加载
obd cluster reload demo
```

# 2023-07-31

```
cd /root/soft/oceanbase-all-in-one/bin
sh install.sh
obd web
# 访问
http://192.168.15.109:8680/
# 安装openjdk
yum install java-1.8.0-openjdk
# 安装的时候要选择更多配置，选择自定义devname,手工填写网卡名称，否则与检查不能通过

http://192.168.15.109:8180/overview   admin/V@kzN%55

obclient -h192.168.15.109 -P2881 -uroot -p'DEny20080!!' -Doceanbase -A
obclient -h192.168.15.109 -P2883 -uroot -p'DEny20080!!' -Doceanbase -A

root@dba   @-#4UTFjr=-rUX0W:hO
```

# 2023-08-01

```
# 优化数据下线备份脚本，将脚本合并为一个脚本，提供下线表清单，自动生成下线脚本及删除脚本。
# 历史库灾备同步数据
```

# 2023-08-02

```
# 提报 oa
1.历史数据归档。完成账务数据历史数据归档工作。完成情况：100%
2.日常邮件处理。完成情况：98%。偶尔有漏执行邮件，通过提醒以后执行完成。因此此项扣2分。
3.开发库 DM 同步任务维护，故障修复。完成情况：100%
4.清理 clickhouse，腾出tidb03根分区空间
5.数据大屏Dataease维护。修改行业系统交易趋势图为组合图，数值进行展示优化。完成情况：100%
6.重新搭建cloud行业库slave从库（10.80.16.97）。完成情况：100%
7.临时表清理及未创建索引主键库表监控。完成情况：100%
8.报表系统维护，维护报表49个，本月无增减。完成情况：100%
9.开发库维护，迁移开发库从山水广场迁移到昌平机房，并将 TiDB 集群迁移到新服务器上。完成情况：100%
10.Yearing开发库使用，SQL审核工作。完成情况：100%。
11.开发库 nacos 数据库迁移。完成情况：100%。
12.搭建 postgresql 测试服务器。完成情况：100%。
13.成都灾备机房数据传输，数据实时同步。完成情况：100%
14.搭建 oceanbase 测试开发服务器。完成情况：100%。
15.清理dba_monitor.processlist运维数据。完成情况：100%。

# mysql延迟备库
stop slave;
change master to master_delay=7200;
start slave;
show slave status\G

# 编写切割合并大文件 python 脚本
# 重装虚拟机
# 升级dataease 到 1.18
```

# 2023-08-03

```
添加历史库 rsync 同步任务，每周一 9 点同步历史库数据
0 9 * * 1 /bin/sh /home/offline_dump/rsync.sh 
# 更新定时报表 未审核且有订单数据
```

# 2023-08-04

```
# 删除 7 天以前的文件
find . -type f -mtime +7 -delete
# 10.80.16.40 ocr文件满导致根空间90%以上，清理7 天前文件
```

# 2023-08-08

```
# oceanbase 创建 2 个租户，dev和 test，导入 2 套数据库
# ocm web控制台界面创建租户
# 创建租户 test    集群名称是demo 默认用户是root，密码是空
obd cluster tenant create myoceanbase -n test --max-cpu=4 --memory-size=4G --log-disk-size=3G --max-iops=10000 --iops-weight=2 --unit-num=1 --charset=utf8mb4

# 登录
obclient -h127.0.0.1 -P2881 -uroot@dev -p'r?p?+91K5yhO7,qQhJ]B)J-PwgcVS{]
' -Doceanbase -A
obclient -h127.0.0.1 -P2881 -uroot@test -Doceanbase -A

# 使用 mydumper 和 myloader 备份和迁移数据
mydumper -h 127.0.0.1 -u root -p PassW0rd_ -B ocean_common -o ./ocean_common/
myloader -h 127.0.0.1 -P 2883 -u 'root@dev#myoceanbase' -p 'yQ_[N;ut|YVRp0xif1z(I_KuF)K7.C' -B ocean_common -o -d ./ocean_common/

# 全库备份
mydumper -h 127.0.0.1 -P 3306 -u 'root' -p 'PassW0rd_' -o ./ocean/
# 
mydumper -h 127.0.0.1 -u root -p PassW0rd_ -B cloud_account -o ./cloud_account/ && tar zcvf cloud_account.tar.gz cloud_account/

tar zxvf cloud_account.tar.gz 
myloader -h 127.0.0.1 -P 2883 -u 'root@dev#myoceanbase' -p 'yQ_[N;ut|YVRp0xif1z(I_KuF)K7.C' -B cloud_account -o -d ./cloud_account/

mydumper -h 127.0.0.1 -u root -p PassW0rd_ -B cloud_risk -o ./cloud_risk/ && tar zcvf cloud_risk.tar.gz cloud_risk/

tar zxvf cloud_risk.tar.gz 
myloader -h 127.0.0.1 -P 2883 -u 'root@dev#myoceanbase' -p 'yQ_[N;ut|YVRp0xif1z(I_KuF)K7.C' -B cloud_risk -o -d ./cloud_risk/

# 优化数据大屏后台程序，修复告警信息更新刷屏问题
# 优化 zabbix 显示，添加事件确认
```

# 2023-08-09

```
# 批量删除，避免 rm -f ./*.aud 提示 too many argument
find . -type f -name 'riskctlstby_ora_*_2021*.aud' -delete

find . -type f -name 'log_*.xml' -delete

# 全库备份
mydumper -h 127.0.0.1 -P 3306 -u 'root' -p 'PassW0rd_' -o ./ocean/
# 全库恢复
myloader -h 127.0.0.1 -P 2883 -u 'root@dev#myoceanbase' -p 'yQ_[N;ut|YVRp0xif1z(I_KuF)K7.C' -o -d ./ocean/

# 运维大屏添加显示慢查询图形
# 修改登录超时时间
/opt/dataease/conf/dataease.properties
#登录超时时间单位min  如果不设置 默认8小时也就是480
dataease.login_timeout=480000
# 重启
dectl restart
```

# 2023-08-10

```
更新定时任务 飞天定时任务-月
```

# 2023-08-11

```
obclient -h192.168.15.109 -P2881 -uroot -p'/11#J],A._,=j!cx]3frM)qKOG=-D' -Doceanbase -A
obclient -h192.168.15.109 -P2883 -uroot -p'/11#J],A._,=j!cx]3frM)qKOG=-D' -Doceanbase -A
http://192.168.15.109:8180  admin   6Ek9l@%R

# 修改日志等级  参数管理里面设置
ALTER SYSTEM SET syslog_level='ERROR' SCOPE = SPFILE;

# 在用 navicat 传数据过程中报错 [ERR] 2> 1499 - Too many partitions
是内存给太少了导致的
[ERR] 4012 - Timeout, query has reached the maximum query timeout: 10000000(us), maybe you can adjust the session variable ob_query_timeout or query_timeout hint, and try again.
set global ob_query_Timeout=36000000000;
```

# 2023-08-14

```
# 安装部署工具，ocp-express 不具备添加主机，zone 等功能
cloud_dev@cloud_dev   c3@CZ#R^n@m6yrpG
?F6~?sYB+W,A%8w
```

# 2023-08-15

```
{e/cMb5f7SlP-tYNr|#P:[

DM 监控任务，添加上游服务器字段，方便快速定位源服务器

mysqlbinlog --start-position=<Relay_Log_Pos> <Relay_Log_File>
mysqlbinlog --start-position=db1-2-relay-bin.040198 431650142
```

# 2023-08-17

```
jupyter notebook --port 8088 --no-browser
jupyter notebook list
jupyter notebook stop [id]

mydumper -h 127.0.0.1 -P 2881 -u 'root@cloud_dev' -p '^n8Szhip|Tg}sao&:[[}4q4QsO/U%s' -o ./cloud/
mysqlbinlog --no-defaults mysql-bin.000025 --base64-output=decode-rows -vv --skip-gtids=true  |grep -C 3 -i 'drop database' 

mysqlbinlog --no-defaults mysql-bin.000025 --base64-output=decode-rows -vv --skip-gtids=true  --database='business_db'  > 2.sql
mysqlbinlog --no-defaults mysql-bin.000026 --base64-output=decode-rows --stop-position=25756362 --database=business_db|mysql -v business_db

mysqlbinlog --no-defaults --stop-datetime "2023-08-17 11:00:00" mysql-bin.000026  | mysql -v business_db
```

# 2023-08-18

```
#  查看 进程 中文件运行的绝对路径  
ls -l /proc/$(pidof Yearning)/exe
lrwxrwxrwx. 1 root root 0 7月  27 10:45 /proc/14042/exe -> /root/Yearning-go/Yearning
```

# 2023-08-21

```
grant all  on business_db.* to 'huangzc'@'%';
mysqldump -uroot -proot --set-gtid-purged=OFF --default-character-set=utf8mb4  business_db | gzip > business_db_`date +%Y%m%d`.sql.gz

*|WRT[7FB!M-d-mFT:8Exx}
```

# 2023-08-22

```
# 使用 mydumper 和 myloader 备份和迁移数据
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B cloud_account -o ./cloud_account/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B cloud_account_flow -o ./cloud_account_flow/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B cloud_capital -o ./cloud_capital/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B cloud_common -o ./cloud_common/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B cloud_marketing -o ./cloud_marketing/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B cloud_merchant -o ./cloud_merchant/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B cloud_order -o ./cloud_order/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B cloud_profit -o ./cloud_profit/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B cloud_risk -o ./cloud_risk/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B nacos -o ./nacos/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B oceanbase -o ./oceanbase/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B petcloudnacos -o ./petcloudnacos/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B petoceannacos -o ./petoceannacos/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B petwalletnacos -o ./petwalletnacos/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B seata -o ./seata/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B testcloudnacos -o ./testcloudnacos/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B testoceannacos -o ./testoceannacos/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B testwalletnacos -o ./testwalletnacos/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B wallet_account -o ./wallet_account/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B wallet_common -o ./wallet_common/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B wallet_order -o ./wallet_order/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B wallet_xxl_job -o ./wallet_xxl_job/
mydumper -h 192.168.15.109 -P 2881 -u root@cloud_dev -p "^n8Szhip|Tg}sao&:[[}4q4QsO/U%s" -B xxl_job -o ./xxl_job/   
myloader -h 127.0.0.1 -P 2883 -u 'root@dev#myoceanbase' -p '^n8Szhip|Tg}sao&:[[}4q4QsO/U%s' -B cloud_account -o -d ./cloud_account/

obclient -h192.168.15.109 -P2881 -uroot -p'/11#J],A._,=j!cx]3frM)qKOG=-D' -Doceanbase -A

obclient -h 192.168.15.109 -P 2881 -u 'root@cloud_dev' -p'^n8Szhip|Tg}sao&:[[}4q4QsO/U%s' -Doceanbase -A
mydumper -h 127.0.0.1 -P 2881 -u 'root@cloud_dev' -p '^n8Szhip|Tg}sao&:[[}4q4QsO/U%s' -o ./cloud/

set global ob_query_Timeout=36000000000;
obclient -h192.168.15.109 -P2881 -uroot -p'6]HGpOkne0S_/SdZ@Nyg^IB#]cUCUI2a' -Doceanbase -A
Zf/[J#HI!oVz.Vj7=idJARI{6L
http://192.168.15.110:8080   admin  6]HGpOkne0S_/SdZ@Nyg^IB#]cUCUI2a

77    数据安全要求审查    数据保护    敏感信息安全审计    4.5.1.2    每年应至少开展两次支付敏感信息安全的内部审计，并形成报告存档备查。    是
83    数据安全要求审查    交易数据以及客户数据的安全性    数据销毁    4.5.3.13    应具有数据销毁制度和相关记录，并实现有效的数据销毁功能。    是
71    数据安全要求审查    数据保护    敏感信息安全审计    4.5.1.2    每年应至少开展两次支付敏感信息安全的内部审计，并形成报告存档备查。    是
业务连续性要求审查    日常维护    定期演练    4.7.4.1    "应制定演练计划，根据不同的应急恢复内容，确定演练的周期，至少每年一次。
应每年进行业务连续性演练，包括主备机房的切换演练，并保存演练记录。
应对演练中暴露出的问题进行总结并及时整改。"    是
```

# 2023-08-24

```
mysqlbinlog --base64-output=decode-rows -vv --start-datetime="2023-08-24 00:30:05"  --database=dba_monitor paas-mysql02-relay-bin.002764> /tmp/dba_monitor.log
mysqlbinlog --base64-output=decode-rows -vv --start-datetime="2023-08-25 00:30:05"  --database=ds_org_merchant mysql-bin.002808 > /tmp/ds_org_merchant.log

mysqlbinlog --base64-output=decode-rows -vv --start-datetime="2023-08-25 08:30:05"  --database=ds_channel_route mysql-bin.002808 > /tmp/ds_channel_route.log
```

# 2023-08-28

```
ocp web 界面（白屏）安装 ob 集群，主机上要安装 net-tools
ocp 主机要安装 mysql     yum install mysql -y
 检查是否有 admin 用户，没有则创建   useradd admin  passwd admin   admin

vim /etc/sysctl.conf
net.core.somaxconn = 2048
net.core.netdev_max_backlog = 10000
net.core.rmem_default = 16777216
net.core.wmem_default = 16777216
net.core.rmem_max = 16777216
net.core.wmem_max = 16777216
net.ipv4.ip_local_port_range = 3500 65535
net.ipv4.ip_forward = 0
net.ipv4.conf.default.rp_filter = 1
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.tcp_syncookies = 0
net.ipv4.tcp_rmem = 4096 87380 16777216
net.ipv4.tcp_wmem = 4096 65536 16777216
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_fin_timeout = 15
net.ipv4.tcp_max_syn_backlog = 16384
net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_slow_start_after_idle=0
vm.swappiness = 0
vm.min_free_kbytes = 2097152
vm.max_map_count=655360
fs.aio-max-nr=1048576

sysctl -p

vi /etc/security/limits.conf
* soft nofile 655360
* hard nofile 655360
* soft nproc 655360
* hard nproc 655360
* soft core unlimited
* hard core unlimited
* soft stack unlimited
* hard stack unlimited

ulimit -a


# 修复 yearning 不能添加多个主机的问题  
(/Users/henryyee/Yearning-go/src/handler/manage/db/impl.go:81) 
[2023-08-28 16:48:07]  Error 1364: Field 'principal' doesn't have a default value 
给 core_data_sources 的 principal 添加默认值 'admin'
alter table core_data_sources modify column principal varchar(150) default  'admin';
```

# 2023-08-29

```
centos7 下只能装 node16，装 node18 会报glibc 版本太低

centos7 适配 golang 下载地址
wget https://mirrors.ustc.edu.cn/golang/go1.19.12.linux-amd64.tar.gz
node-v16.14.2-linux-x64.tar.xz

mac下添加goproxy 到环境变量
echo "export GOPROXY=https://proxy.golang.com.cn,direct" >> ~/.profile && source ~/.profile
echo "export GOPROXY=https://proxy.golang.com.cn,direct" >> ~/.zshrc && source ~/.zshrc  

# 安装依赖包
go get github.com/go-sql-driver/mysql

#  打包到 linux 可执行程序
GOOS=linux GOARCH=amd64 go build -o my_first_go main.go
GOOS=windows GOARCH=amd64 CC=x86_64-w64-mingw32-gcc CGO_ENABLED=1 go build -o my_first_go.exe main.go
```

# 2023-09-01

```
# oceanbase 开启审计(未验证)
enable_sql_audit=True

ALTER SYSTEM enable_sql_audit=True;

obclient -h192.168.15.110 -P2881 -uroot -p'71=_@wk-jH|j+Qa?d}rI(' -Doceanbase -A
[ERROR] failed to start 192.168.15.110 ocp server
```

# 2023-09-04

```
# 绩效考核
1.日常邮件处理。完成情况：98%。偶尔有漏执行邮件，通过提醒以后执行完成。因此此项扣2分。
2.优化数据下线备份脚本，将脚本合并为一个脚本，提供下线表清单，自动生成下线脚本及删除脚本。完成情况：100%。
3.优化 zabbix 显示，添加事件确认。完成情况：100%。
4.历史库灾备同步数据.完成情况：100%
5.数据大屏Dataease维护。修复告警信息更新刷屏问题。运维大屏添加显示慢查询图形。修改登录超时时间。完成情况：100%
6.老手单系统维护。ocr文件满导致根空间90%以上，清理7天前文件。完成情况：100%
7.DM 监控任务，添加上游服务器字段，方便快速定位源服务器。完成情况：100%
8.配合完成检查项目。完成情况：100%
9.添加历史库rsync同步任务，每周一9点同步历史库数据。完成情况：100%
10.报表系统维护，维护报表49个。更新未审核且有订单数据定时报表，更新飞天定时任务-月。完成情况：100%
11.开发库维护，迁移开发库从山水广场迁移到昌平机房，并将 TiDB 集群迁移到新服务器上。完成情况：100%
12.Yearing开发库使用，修复yearning不能添加多个主机的问题。完成情况：100%。
13.搭建 oceanbase  持续测试，搭建 4.2 集成版本，ocp 功能不完整。完成情况：100%。

解决 tidb 报错 [libprotobuf ERROR /tmp/tzg/release-centos7/prepare-environments/grpc/third_party/protobuf/src/google/protobuf/message_lite.cc:406] tipb.SelectResponse exceeded maximum protobuf size of 2GB: 11248412598
flink和tiflash 存在征用，将 flink 和 tiflash 分离部署

#  老收单数据库服务器资源部分回收
```

# 2023-09-05

```
#  只导出库表结构
mysqldump -u root -pabc@123 -S /home/mysql/mysql-files/mysql_3306.sock --set-gtid-purged=OFF --skip-opt ds_account_3 --no-data > ds_account_3.sql

go get github.com/emersion/go-imap
go get github.com/emersion/go-message
go get github.com/emersion/go-imap/client@v1.2.1
```

# 2023-09-06

```
http://192.168.15.225:8261/dashboard/migration/task
dm 同步问题 test_order_clearing 故障处理

王建伟分析和优化 sql
于瑞杰无法登录堡垒机问题

tiup dmctl --master-addr 192.168.15.225:8261 handle-error test_order_clearing skip
tiup dmctl --master-addr 192.168.15.225:8261 query-status test_order_clearing skip
```

# 2023-09-08

```
goproxy
https://goproxy.io
https://goproxy.cn
https://mirrors.aliyun.com/goproxy/
```

# 2023-09-19

```
mysql日志信息如下：
2023-09-19T14:00:04.981706Z 7026 [Note] Aborted connection 7026 to db: 'mysql' user: 'root' host: '10.128.88.7' (Got an error writing communication packets)
修改参数：
net_write_timeout
net_read_timeout
```

# 2023-09-21

```
成都灾备vpn
VPN客户端下载地址：
https://www.hillstonenet.com.cn/support-and-training/hillstone-secure-connect/
用户名：
yinhb/Xdjk@2023!
118.123.206.128   4433

#  go 打包到 linux 可执行程序
GOOS=linux GOARCH=amd64 go build -o my_first_go main.go
# go 打包到 windows 可执行程序
GOOS=windows GOARCH=amd64 CC=x86_64-w64-mingw32-gcc CGO_ENABLED=1 go build -o *.go
# go 打包到macos下可执行程序
GOOS=darwin GOARCH=amd64 go build -o my_first_go *.go
```

# 2023-09-25

```
/bin/zsh -c "$(curl -fsSL https://gitee.com/cunkai/HomebrewCN/raw/master/Homebrew.sh)"

暂时关闭反洗钱55,56服务器binlog配置
```

# 2023-09-26

```
command+j 复制图层，关闭原始图层眼睛
矩形框选水面位置-alt+图层蒙版
command+j 复制图层
command+t 自由变换，反转，拖动图片与水面对齐
最上面的图层转化为智能对象，双击智能对象，进入到智能对象
新建一个图层， alt+delete填充黑色
右键单击黑色图层-复制图层-文档（新建，名称置换）
在新文档中，滤镜->杂色->添加杂色（数量最大，平均分布，单色）
滤镜-模糊-高斯模糊1.4
通道-红色-滤镜-风格化-浮雕效果（角度180，高度1，数量最大）
通道-绿色-滤镜-风格化-浮雕效果（角度90，高度1，数量最大）
点击rgb通道-图层-滑动滚轮缩小-command+t-鼠标右键-透视-拖动右下角控制块往外拖
command+图层缩览图（就是载入选区）-图层蒙版-转化为智能对象-保存psd格式
回到图层1文档-删除新建图层1
滤镜-模糊-动感模糊（90度，距离10）
滤镜-扭曲-置换（水平10，垂直150）-找到前面保存的psd文件-command+s保存
回到最初的文档，打开眼睛，搞定

# 7-9 季度绩效考核
1.日常邮件处理。完成情况：98%。偶尔有漏执行邮件，通过提醒以后执行完成。因此此项扣2分。
2.优化数据下线备份脚本，将脚本合并为一个脚本，提供下线表清单，自动生成下线脚本及删除脚本。完成情况：100%。
3.优化zabbix部分显示调整，视图提示功能调整。完成情况：100%。
4.历史库灾备同步数据.完成情况：100%
5.反洗钱系统维护，优化配置项，缓解存储空间告警问题。完成情况：100%。
6.数据大屏Dataease维护。dataease升级到18版本。修复告警信息更新刷屏问题。运维大屏添加显示慢查询图形。修改登录超时时间。完成情况：100%
7.老手单系统维护。下线收单，互联网，金盈信，风控dg节点。完成情况：100%
8.DM服务器维护，开发库及生产库DM服务器任务异常修复。完成情况：100%
9.mysql参数测试优化，测试mysql超时参数 net_write_timeout,net_read_timeout的作用。完成情况：100%
10.配合完成各项检查项目。完成情况：100%
11.添加历史库rsync同步任务，每周一9点同步历史库数据。完成情况：100%
12.报表系统维护，维护报表49个。更新未审核且有订单数据定时报表，更新飞天定时任务-月。完成情况：100%
13.开发库维护，迁移开发库从山水广场迁移到昌平机房，并将 TiDB 集群迁移到新服务器上。完成情况：100%
14.Yearing开发库使用，修复yearning不能添加多个主机的问题。完成情况：100%。
15.搭建 oceanbase  持续测试，搭建 4.2 集成版本，ocp 功能不完整。完成情况：100%。
16.分析优化生产SQL。完成情况：100%。
17.新建业务拆分库。完成情况：100%。
18.迁移跨境数据库，新建一套MySQL主从。完成情况：100%。
19.配合搭建专家库项目所涉及的MySQL数据库。完成情况：100%。

macos - 如何让 Mac ".command"文件在运行 shell 脚本后自动退出？
在脚本末尾添加
osascript -e 'tell application "Terminal" to quit' &
exit
```

# 2023-10-07

```
ps重复上一批操作
alt+command+shift+t
套索全选-command+j将选中部分新建图层
按住command键 选择图层中的缩略图可以选择图层选区,按住shift+command选择图层中的缩略图可以同时选中多个选区
按中括号可以改变橡皮擦的大小
```

# 2023-10-8

```
# 修改tidb查询使用内存量
select @@tidb_mem_quota_query
# 设置成10G
set @@tidb_mem_quota_query=10737418240;
# 语句最长执行时间
select @@global.max_execution_time;
```

# 2023-10-10

```
处理posp监听日志文件过大告警
配合中清协检查数据处理
配合处理商户数据报备
```

# 2023-10-16

```
# 解决测试dm任务报错问题.
```

# 2023-10-17

```
# 查看表容量
select
table_schema as '数据库',
table_name as '表名',
table_rows as '记录数',
truncate(data_length/1024/1024, 2) as '数据容量(MB)',
truncate(index_length/1024/1024, 2) as '索引容量(MB)'
from information_schema.tables
order by data_length desc, index_length desc;

# 查询单库表容量
select
table_schema as '数据库',
table_name as '表名',
table_rows as '记录数',
truncate(data_length/1024/1024, 2) as '数据容量(MB)',
truncate(index_length/1024/1024, 2) as '索引容量(MB)'
from information_schema.tables
where table_schema='mysql'
order by data_length desc, index_length desc;

# 数据导出用法
mysqldump --default-character-set=utf8mb4 --master-data=2 --single-transaction --set-gtid-purged=off --hex-blob --no-create-info --skip-add-drop-table --skip-add-locks --tables dbname tablename

--master-data=2 参数会在备份期间对所有表加锁 FLUSH TABLES WITH READ LOCK，并执行 SHOW MASTER STATUS 语句以获取二进制日志信息。因此，在备份期间可能会影响数据库的并发性能。如果您不需要进行主从复制，则可以考虑不使用 --master-data=2 参数。
--single-transaction 参数用于在备份期间“使用事务来确保数据一致性”，从而避免在备份期间锁定表。[必须有]

// 部分数据导出追加参数
--where="create_time>'2023-01-02'"
// 可选不导出表结构，
--no-create-info --skip-add-drop-database --skip-add-drop-table

# 导出为csv
/data/mysql/3306/base/bin/# mysqldump -uadmin -p123456 -P3306 -h127.0.0.1 --default-character-set=utf8mb4 --single-transaction --set-gtid-purged=OFF  --triggers --routines --events --hex-blob --fields-terminated-by=',' --fields-enclosed-by='"' --lines-terminated-by='\n'  -T /data/mysql/3306/tmp test

//其中 test 后面也可以指定表名，不指定就是全库。
test t_order_info t_order_info01
其中 --single-transaction --set-gtid-purged=OFF  --triggers --routines --events --hex-blob 
为了防止提示，可选

# 多线程导入导出
mydumper -u admin -p 123456 -P 3306 -h 127.0.0.1 -t 8 --trx-consistency-only -G -E -R --skip-tz-utc --verbose=3 --compress --no-schemas --rows=1000000  -T test.t_order_info  -o /backup

// 导出时支持部分导出追加参数

--where="create_time>'2023-01-02'"

// 文件输出
test01.t_order_info.00000.dat # 包含 CSV 数据
test01.t_order_info.00000.sql # 包含 LOAD DATA 语句

// 导入命令
LOAD DATA LOCAL INFILE '/data/mysql/3306/tmp/test01.t_order_info.00005.dat' REPLACE INTO TABLE `t_order_info` CHARACTER SET binary FIELDS TERMINATED BY ',' ENCLOSED BY '"' ESCAPED BY '\\' LINES STARTING BY '' TERMINATED BY '\n' (`ID`,`order_no`,`order_status`,`flag`,`create_time`,`modify_time`);

myloader -u admin -p 123456 -P 3306 -h 127.0.0.1 --enable-binlog -t 8 --verbose=3 -B test -d /backup

//  导入主库时需要添加 
--enable-binlog

// 库名可以自定义





-B test 

# 凌晨两点跑批任务导致主备延迟原因排查
# 专家库数据库备份方案及部署
# xxl_job 开发库从222拷贝到194命名为 paas_xxl_job
```

# 2023-10-18

```
(?=.*pattern) 是正则表达式中的一个构造，被称为正向肯定预查,可以理解为"至少存在"
```

# 2023-10-26

```
余兴龙 定制报表开发
```

# 2023-10-27

```
反洗钱库zeus_2,4迁移到60服务器
```

# 2023-10-30

```
绩效,本月工作总结.
1.日常邮件处理。完成情况：98%。邮件处理难免会有漏执行的情况，通过提醒以后执行完成。因此此项扣2分。
2.tidb内存参数优化调整,调整查询,执行时长限额。完成情况：100%。
3.posp监听日志过大问题处理。完成情况：100%。
4.配合中清协检查数据处理.完成情况：100%
5.配合处理商户数据报备。完成情况：100%
6.老手单系统维护。ocr文件满导致根空间90%以上，清理7天前文件。完成情况：100%
7.DM 监控任务，添加上游服务器字段，方便快速定位源服务器。完成情况：100%
8.配合完成检查项目。完成情况：100%
9.解决测试dm任务报错问题.完成情况：100%
10.报表系统维护，维护报表53个。新增厦门人行数据明细和汇总定时报表。完成情况：100%
11.开发库基础维护。完成情况：100%
12.Yearing开发db审核基础工作。完成情况：100%。
13.xxl_job开发库迁移。完成情况：100%。
14.凌晨两点主备延迟原因排查为备份任务所致,已错开跑批任务和备份时间.

# mysqldump 备份单库
mysqldump -uroot -pqpIRWHBkum -P3430 --set-gtid-purged=OFF   zeus_4 | gzip > zeus_4_`date +%Y%m%d`.sql.gz

# 恢复单库
gunzip < zeus_4_`date +%Y%m%d`.sql.gz | mysql -uroot -pqpIRWHBkum -P3430 zeus_4

# 地面硬化
地砖:48平米 800*800 75片 + 5(余量)
卫生间:10.8+7.2=18平米
厨房: 5.2+3+8.4=16.6平米
总共:18+16.6=34.6 平米  300*600 需要 192片+3(余量)
卫生间地面: 1.5  300*300  16片+1(余量)

总共砖: 80(大)+212(小)
```

2023-11-02

```
# mysqldump 备份单库
mysqldump -uroot -pqpIRWHBkum -P3430 --set-gtid-purged=OFF --single-transaction  zeus_4 | gzip > zeus_4_`date +%Y%m%d`.sql.gz
# mysqldump 恢复,500G大约需要20多个小时
gunzip < zeus_4_*.sql.gz |mysql -uroot -p zeus_4
5yahldqm
192.168.16.92 开发库故障处理 日志占满

添加定时任务  小微规则统计
lizg@mfhcd.com,luobs@mfhcd.com,wanglan@mfhcd.com,mashuo@mfhcd.com,liumin@mfhcd.com,hancl@mfhcd.com,liupp@mfhcd.com,yanmeng@mfhcd.com

mysqldump -uroot -pqpIRWHBkum -P3430 --set-gtid-purged=OFF --single-transaction  zeus_2 | gzip > zeus_2_`date +%Y%m%d`.sql.gz
gunzip < zeus_2_*.sql.gz |mysql -uroot -p zeus_2
```

# 2023-11-06

```
# 反洗钱库迁移 10.80.16.55/56 zeus_2,zeus_4迁移到10.80.16.60
mysqldump -uroot -pqpIRWHBkum -P3430 --set-gtid-purged=OFF --single-transaction  zeus_2 ind_data_cal_ctif_id t_stan_stif | gzip > zeus_2_1_`date +%Y%m%d`.sql.gz
gunzip < zeus_2_1_*.sql.gz |mysql -uroot -p zeus_2
5yahldqm

# 重新搭建10.80.16.97 could备库,测试硬件故障是否修复

# 创建新虚拟环境
conda create -n myenv310 python=3.10
conda env list
conda activate myenv310
# 查看conda 信息
conda info

# 修改conda虚拟环境保存路径
vi .condarc
envs_dirs:
  - /root/envs
```

# 2023-11-07

```
# 添加定时报表   259改造监控日报 ，烦请添加定时任务，每日8点30 执行
# 升级报表服务器python版本3.9 -> 3.11
# 优化定时报表脚本  exports.py 去掉压缩包中的多层级目录 
# 优化tidb统计信息收集脚本,部分频繁变更表收集周期改成按周收集

# tidb任务报错,经查发现是新增了其他表导致匹配规则过大,重新修改规则,删除老任务,新建任务,覆盖数据
  "error_msg": "tikv aborts txn: Value with size 4606511 exceeded ValueLogFileSize 
  "name": "order_clearing_new",
  "source_name": "cloud",
  "stage": "Paused",
```

# 2023-11-09

```
# 迁移反洗钱zeus_2库
mysqldump -uroot -pqpIRWHBkum -P3430 --set-gtid-purged=OFF --single-transaction  zeus_2 | gzip > zeus_2_1_`date +%Y%m%d`.sql.gz
gunzip < zeus_2_1_*.sql.gz |mysql -uroot -p zeus_2
5yahldqm
```

# 2023-11-10

```
优化定时报表程序功能,分卷压缩邮件标题增加序号,去掉文件层级目录

# 两句配置log输出到文件
import logging
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s %(levelname)s: %(message)s',
                    filename='example.log', filemode='w')
logging.debug('This is a debug message')
```

# 2023-11-13

```
# 更新定时报表系统,优化邮件发送模块和日志模块
使用print(pkg_name.__file__) 可以查看该包所在的路径

# zmail
def sendmail(subject, content_text, attach_files, receiver):
    send_info = {
        'subject': subject,
        'content_text': content_text,
        'attachments': attach_files,
    }
    to_email = receiver
    server = zmail.server(config.get("user"), config.get("passwd"),
                          smtp_host=config.get("smtp_server"),
                          smtp_ssl=config.get("ssl"),
                          smtp_port=config.get("port"))
    server.send_mail(to_email, send_info)
```

# 2023-11-14

```
# 测试库mysql宕机修复
# 测试库dm任务挂起原因分析
```

# 2023-11-21

```
# mac安装pytorch,cpu版本
conda install pytorch torchvision -c pytorch
```

# 2023-11-27

```
# 批量删除，避免 rm -f ./*.aud 提示 too many argument
find . -type f -name 'riskctlstby_ora_*_2021*.aud' -delete
# 互联网webpay库审计日志清理
```

# 2023-11-28

```
# 按字节拆分
split -b 100m mer_xw_report3.7z 
# 按行数拆分
split -l 1000000 1111.log
# linux下合并
cat x* > mer_xw_report3.7z 
# windows合并
copy /b x* mer_xw_report3.7z 
```

# 2023-11-29

```
# 测试库mysql故障恢复
```

# 2023-12-04

```
1.日常邮件处理。完成情况：98%。邮件处理难免会有漏执行的情况，通过提醒以后执行完成。因此此项扣2分。
2.反洗钱库迁移 10.80.16.55/56 zeus_2,zeus_4迁移到10.80.16.60
3.添加定时任务(小微规则统计).259改造监控日报
4.重新搭建10.80.16.97 could备库,测试硬件故障是否修复.
5.升级报表服务器python版本3.9 -> 3.11
6.优化定时报表程序功能,分卷压缩邮件标题增加序号,去掉文件层级目录
7.优化tidb统计信息收集脚本,部分频繁变更表收集周期改成按周收集
8.互联网webpay库审计日志清理
9.测试库mysql宕机修复
10.测试库dm任务挂起原因分析
11.开发库基础维护。完成情况：100%
12.Yearing开发db审核基础工作。完成情况：100%。
13.xxl_job开发库迁移。完成情况：100%。
```

# 2023-12-07

```
处理tidb03空间告警问题
```

# 2023-12-08

```
测试库tidb-dm无法创建任务故障处理,缓存清理
咖啡默认密码1609
```

# 2023-12-11

```
# 配合updss检查
# 添加dd备份任务监控脚本  10.83.5.23
```

# 2023-12-13

```
业务拆分
添加备份监控
添加704交易监控
```

# 2023-12-14

```
mysql中将字符串转成时间
SELECT TIME_FORMAT('000012', '%H:%i:%s'); 
```

# 2023-12-19

```
WiFi名: PaaS-LOG-5G
密码: Xdjk@2020
```

# 2024-01-08

```
定时任务收件人调整
阿里云域名注册地址
地址：https://wanwang.aliyun.com/
用户名：31551158
密码：xdjk6597507023
630100
# 阿里云监控行业交易成功率
```

# 2024-01-09

```
# python进程不显示具体脚本名称,只显示python.exe ,通过下面方法可以找到对应的进程并结束进程
wmic process where name="python.exe" get processid,commandline
taskkill /pid  12740 /f

# 批量杀死所有python进程
wmic process where name="python.exe" call terminate

# 等保检查
# windows下curl下载地址 https://curl.se/windows/
# 查看xxljob任务状态(什么也不显示)
curl -X GET http://127.0.0.1:8080/xxl-job-admin/jobinfo/load?jobId=87
curl -X POST http://127.0.0.1:8080/xxl-job-admin/glueJobInfo/load?jobId=87
```

# 2024-01-12

```
# 结算连接数优化
# 监控交易成功率脚本bug修复和脚本优化
```

# 2024-01-15

```
zcat cis_order_pay_info_pos_2022_1.sql.gz | mysql dbname
```

# 2024-01-16

```
tiup dmctl --master-addr 192.168.15.225:8261 handle-error test_order_clearing skip

# 批量跳过有错误
dm_master=192.168.15.225:8261;for i in `tiup dmctl --master-addr $dm_master query-status |grep taskName|awk '{print $2}'|cut -d, -f1|cut -d'"' -f2`;do tiup dmctl --master-addr $dm_master handle-error $i skip ; done

mysql-bin.020706, 242922187
show binlog events in 'mysql-bin.020726' from 1041998421 limit 100;

mysqlbinlog --no-defaults mysql-bin.020706 --base64-output=decode-rows --start-position=242920584 --database=ds_cscb

tiup dmctl --master-addr 10.80.17.19:8261 handle-error ds_capital_settle_order_pos_2024*] skip

mysql-bin.020726, 1041998421
mysqlbinlog --no-defaults mysql-bin.020726 --start-position=1041998421  --stop-position=1041999662 --base64-output=decode-rows -vv --skip-gtids=true 

for i in `mysql -e 'show processlist'|grep -v 'Sleep'|grep 'DELETE'|awk '{print $1}'`;do mysql -e "kill $i;";done
```

# 2024-01-19

```
优化定时报表
```

# 2024-01-22

```
增加 环球通结算单日报
```

# 2024-01-23

```
jupyter中的魔法命令   # 启动jupyter jupyter notebook
%run
%timeit
%%time

# 查找连接池大的应用
select data_id,content from nacos_paas.config_info where (content like '%maximum-pool-size=30%' or content like '%maximum-pool-size=50%' or content like '%maximum-pool-size=100%')
and ( content like '%ds_capital%' 
or content like '%ds_cscb%' 
or content like '%profit%' 
or content like '%profit_optimize%' 
or content like '%maxwell%' 
)

nparr = np.array([i for i in range(50)])
# 创建全0矩阵，默认是float
np.zeros(10)
np.zeros(10, dtype=int)
# 创建3行5列的矩阵
np.zeros((3,5))
# 创建全1矩阵,值都是666
np.ones((3,6), 666)
# 生成等差数列
np.linspace(0,20,10)
# 随机数矩阵
x = np.random.randint(4,8,[4,5])
# 矩阵取子矩阵
subx = x[:2,:3].copy() # 如果不用copy，则子矩阵和原矩阵有关联

# 创建一维数组
x = np.arange(10)

A = np.arange(16).reshape((4,4))

# 垂直分割矩阵
A1,A2 = np.split(A, [2], axis=1) # axis=0 横向
或者
A1,A2 = np.vsplit(A, [2])
```

# 2024-01-24

```
ANALYZE TABLE profit.profit_real_time_payment_core;
耗时16s

# 批量跳过dm中的ddl错误
dm_master=192.168.15.225:8261;for i in `tiup dmctl --master-addr $dm_master query-status |grep taskName|awk '{print $2}'|cut -d, -f1|cut -d'"' -f2`;do tiup dmctl --master-addr $dm_master handle-error $i skip ; done
```

# 2024-01-26

```
# 在python的类中，不是用户传入的变量，而是通过计算出来的变量，变量名用末尾加_
```

# 2024-01-30

```
uuid_short() 可以在 insert into xxx select * from xxx 中使用来生成某一列的唯一值
```

# 2024-02-01

```
线性回归算法评价指标： MSE/RMSE/MAE
# 调试行业flinksql无法抓取数据问题

工作总结
1.日常邮件处理。完成情况：98%。邮件处理难免会有漏执行的情况，通过提醒以后执行完成。因此此项扣2分。
2.添加定时任务(环球通结算单数据).完成情况：100%
3.调试行业flinksql无法抓取数据问题。完成情况：100%
4.优化定时报表，合并zip和no_zip，更新版本到v4。完成情况：100%
5.结算连接数优化。完成情况：100%
6.监控交易成功率脚本bug修复和脚本优化。完成情况：100%
7.全库清理无主键库表，优化数据库体积和性能。该工作将持续进行。完成情况：100%
8.监控介入阿里云，监控行业交易成功率。完成情况：100%
9.配合完成等保检查。完成情况：100%

# debezium配置同步需要配置如下权限
grant replication slave, replication client on *.* to 'debezium'@'%';
```

# 2024-02-02

```
# 用yum安装的openjdk的安装路径在 /usr/lib/jvm下
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.372.b07-1.el7_9.x86_64/jre/

# 配置seatunnel环境变量
vi /etc/profile.d/seatunnel.sh
JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.372.b07-1.el7_9.x86_64/jre/
CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
export JAVA_HOME CLASSPATH

export SEATUNNEL_HOME=/home/seatunnel/backend
export PATH=$PATH:$JAVA_HOME/bin:$SEATUNNEL_HOME/bin

source /etc/profile.d/seatunnel.sh
# 安装插件
sh bin/install-plugin.sh 
sudo ./bin/seatunnel.sh --config ./config/v2.batch.config.template -e local
nohup sh bin/seatunnel-cluster.sh 2>&1 &

dm_master=10.80.17.19:8261;for i in `tiup dmctl --master-addr $dm_master query-status |grep taskName|awk '{print $2}'|cut -d, -f1|cut -d'"' -f2`;do tiup dmctl --master-addr $dm_master handle-error $i skip ; done

tiup dmctl --master-addr 10.80.17.19:8261 handle-error pos_mer_surcharges_rate_trade skip

tiup dmctl --master-addr 10.80.17.19:8261 query-status pos_mer_surcharges_rate_trade
```

# 2024-02-04

```
# 更新飞天定时任务
# 添加重复出款定时任务
# 添加商户两小时交易定时任务
```

# 2024-02-18

```
需要自己准备download_datasource-mvn.sh文件并下载相关的jar包，并将jar包放置到libs目录下

# 后端启动
nohup sh bin/seatunnel-cluster.sh 2>&1 &
# web端启动
sh bin/seatunnel-backend-daemon.sh start

测试seatunnel的安装
经过测试，服务可以正常启动，但是功能似乎有些问题，无法保存同步任务

sh ./bin/seatunnel.sh --config ./config/seatunnel.mysql.conf.template -e local

ln(10 ^2) = 2 = 2* ln10 = 2 * 1
```

# 2024-02-22

```
准备分类评级材料
```

# 2024-03-04

```
# 监控告警介入微信小程序通知
```

# 2024-03-05

```
# 更新飞天定时任务
# 添加重复出款定时任务
# 添加商户两小时交易定时任务
# 准备分类评级材料
# 测试seatunnel的安装和使用
# 解决调试行业flinksql无法抓取数据问题，权限设置缺失。

# 提报绩效
1.日常邮件处理。完成情况：98%。邮件处理难免会有漏执行的情况，通过提醒以后执行完成。因此此项扣2分。
2.添加重复出款定时任务.完成情况：100%
3.添加商户两小时交易定时任务.完成情况：100%
4.更新飞天定时任务.完成情况：100%
5.准备分类评级材料,配合分类评级检查。完成情况：100%
6.测试seatunnel的安装和使用（不完善，功能bug）。完成情况：100%
7.解决调试行业flinksql无法抓取数据问题，权限设置缺失。完成情况：100%
8.监控告警介入微信小程序通知。完成情况：100%


# 中国支付清算协会自律管理评价涉及的文档更新
# 创建定时任务，tidb库每月1日下午收集大表统计信息

# 创建临时表，通过uuid_short()生成随机数作为主键
CREATE TEMPORARY TABLE temp_table (
  id varchar(64) primary key,
  name VARCHAR(50)
);

insert into temp_table(id,name) select uuid_short() ,province from paas_new_area;

select *,count(*) from  temp_table group by id  having count(*)>1;
```

# 2024-03-06

```
测试阿里云监控告警是否异常
修复备份检查脚本bug
```

# 2024-03-07

```
pyinstaller -D -F -w --target-arch=x86_64 maze_game.py
--target-arch=x86_64
--target-arch=arm64
--target-arch=universal2

# 恢复历史数据 gzip文件恢复,无需解压，中文出现乱码，经过检查，是导出的时候就出现了问题
gunzip < cis_order_pay_inf_scan_2022_12.sql.gz | mysql -uroot -pF98dbSfaWZC8rpz8 -P3306 ds_cis

# 指定字符集
gunzip < cis_order_pay_inf_scan_2022_12.sql.gz | mysql -uroot -pF98dbSfaWZC8rpz8 -P3306 --default-character-set=utf8 ds_cis
```

# 2024-03-08

```
mac安装docker
brew install --cask docker

# docker安装jira
docker network create jira-network
docker run -d --name jira --network jira-network -p 8080:8080 wodby/jira
```

# 2024-03-11

```
# 备份时因为 default-character-set 设置不当导致导出的数据中文全部变成了？，无法恢复
# 查看源表字符集，然后导出时设置字符集  character_set_client 设置不对会影响导出的数据中文转变成问号，如果在.my.cnf中指定了 default-character-set = utf8mb4，且不对后果很严重
## 如果在.my.cnf中没有指定，可以正常导出正常数据
mysqldump -uroot -pF98dbSfaWZC8rpz8 --set-gtid-purged=OFF --default-character-set=utf8mb4 ds_cscb test | gzip > test_`date +%Y%m%d`.sql.gz

ds_cis
cis_order_pay_inf 2022.1-12   省市，银行名称，人名
cis_order_pay_inf_scan_2022_1-12   tidb(2022-05-01 00:00:00)  订单状态和订单描述 
cis_orders_fsas_info_2021_10-2022.12  状态
cis_unionpay_cup_z_2021_1-2022.12  单位名称 card_name_loc
cis_order_pay_info_pos_2021_1-12     省市，银行名称，账户名
cis_order_pay_info_api_2021_1-12     账户名，备注
union_pay_check_bill_detail_2022.1-12  账户名，商户名 acct_name
profit
profit_pos_merc_settle_2020_7-2021.12  tidb 无数据   bi  2021年数据
profit_pos_merc_trade_2020_7-2021-12 tidb无数据       bi  2021年数据
profit_real_time_payment_2020_7-2021-12  tidb无数据   bi 2021年数据
profit_real_time_payment_his_bak  tidb中无表
ds_cscb
set_order_operation_his_2020_1-2021.12 tidb中无表
settle_order_pos_2020.1-2021.12  tidb无数据    bi 2021年数据

mysqldump -uroot -pDCH8Tztd --set-gtid-purged=OFF --default-character-set=utf8mb4 bi_profit bi_profit_pos_merc_settle_2021_9 | gzip > bi_profit_pos_merc_settle_2021_9_`date +%Y%m%d`.sql.gz

gunzip < cis_order_pay_inf_scan_2022_1.sql.gz | mysql -uroot -pF98dbSfaWZC8rpz8 -P3306 dba_monitor

gunzip -c cis_order_pay_inf_scan_2022_10.sql.gz | iconv -f latin1 -t utf8 | mysql -uroot -pF98dbSfaWZC8rpz8 -D dba_monitor

# otrs
http://192.168.16.123/otrs/index.pl
root@localhost  PassW0rd_
```

# 2024-03-12

```
# 大模型
# 安装 langchain-chatchat
docker run -d --gpus all -p 80:8501 registry.cn-beijing.aliyuncs.com/chatchat/chatchat:0.2.7

# 拉取仓库
git clone https://github.com/chatchat-space/Langchain-Chatchat.git

# 进入目录
cd Langchain-Chatchat

# 安装全部依赖
pip install -r requirements.txt 
pip install -r requirements_api.txt
pip install -r requirements_webui.txt  

# 默认依赖包括基本运行环境（FAISS向量库）。如果要使用 milvus/pg_vector 等向量库，请将 requirements.txt 中相应依赖取消注释再安装。

brew install git-lfs
git lfs install
git clone https://huggingface.co/THUDM/chatglm3-6b
git clone https://huggingface.co/BAAI/bge-large-zh

python copy_config_example.py
python init_database.py --recreate-vs

python startup.py -a


# 工单审核
github.com/hhyo/Archery
```

# 2024-03-13

```
brew install docker-compose
# 修复成都灾备库数据同步


gunzip < cis_order_pay_inf_scan_2022_1.sql.gz | mysql -uroot -pF98dbSfaWZC8rpz8 -P3306 ds_cis
gunzip < cis_order_pay_inf_scan_2022_2.sql.gz | mysql -uroot -pF98dbSfaWZC8rpz8 -P3306 ds_cis
gunzip < cis_order_pay_inf_scan_2022_3.sql.gz | mysql -uroot -pF98dbSfaWZC8rpz8 -P3306 ds_cis
gunzip < cis_order_pay_inf_scan_2022_4.sql.gz | mysql -uroot -pF98dbSfaWZC8rpz8 -P3306 ds_cis
gunzip < cis_order_pay_inf_scan_2022_5.sql.gz | mysql -uroot -pF98dbSfaWZC8rpz8 -P3306 ds_cis
gunzip < cis_order_pay_inf_scan_2022_6.sql.gz | mysql -uroot -pF98dbSfaWZC8rpz8 -P3306 ds_cis
gunzip < cis_order_pay_inf_scan_2022_7.sql.gz | mysql -uroot -pF98dbSfaWZC8rpz8 -P3306 ds_cis
gunzip < cis_order_pay_inf_scan_2022_8.sql.gz | mysql -uroot -pF98dbSfaWZC8rpz8 -P3306 ds_cis
gunzip < cis_order_pay_inf_scan_2022_9.sql.gz | mysql -uroot -pF98dbSfaWZC8rpz8 -P3306 ds_cis
gunzip < cis_order_pay_inf_scan_2022_10.sql.gz | mysql -uroot -pF98dbSfaWZC8rpz8 -P3306 ds_cis
gunzip < cis_order_pay_inf_scan_2022_11.sql.gz | mysql -uroot -pF98dbSfaWZC8rpz8 -P3306 ds_cis
```

# 2024-03-14

```
修改 goinception.py 中的 execute_check 函数
# inception_result = self.query(sql=inception_sql)
inception_result = None
 # for r in inception_result.rows:
        #     check_result.rows += [ReviewResult(inception_result=r)]
        #     if r[2] == 1:  # 警告
        #         check_result.warning_count += 1
        #     elif r[2] == 2:  # 错误
        #         check_result.error_count += 1
        #     # 没有找出DDL语句的才继续执行此判断
        #     if check_result.syntax_type == 2:
        #         if get_syntax_type(r[5], parser=False, db_type="mysql") == "DDL":
        #             check_result.syntax_type = 1

        check_result.column_list = ('a', 'b', 'c')
        check_result.checked = True
        check_result.error = None
        check_result.warning = None     
```

# 2024-03-15

```
./goInception -config=config/config.toml

安装pipenv   
pip install pipenv

安装指定版本 
pipenv --python 3.9
激活
pipenv shell

根据包目录安装包
pipenv install -r requirement.txt

配置虚拟环境创建到当前文件夹下，会创建一个.venv的文件夹
vi ~/.bash_profile
export PIPENV_VENV_IN_PROJECT=1

# 安装redis
sudo yum install epel-release
sudo yum install redis
sudo systemctl status redis
sudo systemctl start redis
sudo systemctl enable redis
redis-cli


wget "https://github.com/hhyo/archery/archive/v1.10.0.tar.gz"
 tar zxvf v1.10.0.tar.gz 

# 安装系统依赖
yum -y install gcc gcc-c++ python-devel mysql-devel openldap-devel unixODBC-devel gettext

# 安装依赖库
cd Archery-1.10.0
注释掉 requirements.txt 中的 mysqlclient,添加 pymysql
pip install -r requirements.txt -i https://mirrors.ustc.edu.cn/pypi/web/simple/ 

# 如果使用yum安装的python3，安装mysqlclient可能提示提示缺少Python.h，可安装
yum -y install python36-devel

修改项目中archery/__init__.py ,添加如下内容
import pymysql
pymysql.version_info = (1, 4, 0, "final", 0)  # 这一步是为了避免与 Django 版本兼容性问题
pymysql.install_as_MySQLdb()

# 创建好archery数据库
# 启动goInception

配置archery/settings.py

# 修改数据库用户密码
SET PASSWORD FOR 'root'@'127.0.0.1' = PASSWORD('PassW0rd_');
flush privileges;
alter user 'root'@'127.0.0.1' identified by 'PassW0rd_';

# 数据库初始化
python manage.py makemigrations sql
python manage.py migrate 

# 数据初始化
python manage.py dbshell<sql/fixtures/auth_group.sql
python manage.py dbshell<src/init_sql/mysql_slow_query_review.sql

# 创建管理用户
python3 manage.py createsuperuser

nohup python manage.py qcluster &
#启动服务
nohup python manage.py runserver 0.0.0.0:9123  --insecure  &

现代金控SQL工单系统
修改.env.list 为 .env
```

# 2024-03-19

```
# 远程备库监控
#!/bin/bash

log_path="/root/bin/log/monitor.log"

# Get current date in the specified format (YYYYMMDDHHMMSS)
current_date=$(date "+%Y%m%d%H%M%S")

# Get disk usage for /home/mysql
disk_used=$(df -h | grep /home/mysql | awk '{print $5}')

# Get MySQL slave status
slave_status=$(/usr/local/mysql-5.7.32-linux-glibc2.12-x86_64/bin/mysql -uroot -pF98dbSfaWZC8rpz8 -e 'show slave status\G' 2>/dev/null | grep 'Running' | head -2 | awk '{print $2}' | tr '\n' ' ')

# Output with date, disk usage, and slave status
echo "${current_date}" > $log_path
echo "disk_used: ${disk_used}" >> $log_path
echo "slave_status: ${slave_status}" >> $log_path

sleep 5

rsync -vzrtopg --progress --delete  $log_path sysyunwei@10.80.16.63:/tmp/monitor_21.log

ssh-keygen -t rsa -N ''
ssh-copy-id .ssh/id_rsa.pub sysyunwei@10.80.16.63
okNH36ppke



[root@paas-mysql01 bin]# cat monitor_backup 
#!/bin/bash

# Function to insert monitor information into MySQL database
insert_monitor_info() {
    # Checking if required arguments are provided
    if [ $# -ne 2 ]; then
        echo "Usage: insert_monitor_info <server_ip> <info_file>"
        return 1
    fi

    local server_ip="$1"
    local info_file="$2"

    # Extracting information from the file
    update_time=$(head -n 1 "$info_file")
    disk_usage=$(grep "disk_used" "$info_file" | awk -F ": " '{print $2}')
    slave_status=$(grep "slave_status" "$info_file" | awk -F ": " '{print $2}')

    # Inserting information into MySQL database
    mysql -uroot -pF98dbSfaWZC8rpz8 -h10.80.16.79 -e "replace INTO dba_monitor.backup_server_status (server_ip, update_time, disk_usage, slave_status) VALUES ('$server_ip', '$update_time', '$disk_usage', '$slave_status');"
}

# Calling the function with server IP and file path as arguments
insert_monitor_info '10.33.2.1' '/tmp/monitor_21.log'
```

# 2024-03-20

```
https://110.43.212.129
```

# 2024-03-21

```
GeminiAPIkey
AIzaSyBzaiqIQE6wlH8lifjhzmz-rKqFq3gUSJU

debug = __import__('test_pre').debug
```

# 2024-03-28

```
# 按照大小排序
ls -lShBr | grep -v '^d'

# 部署灾备监控
# 解决DM同步故障，当出现duplicate primary key的情况下，可以删除任务，然后重建一样配置的dm任务即可恢复
# 修复监控networker备份任务脚本
# 测试字符集对文件大小的影响 general_ci 和 bin的区别

# 富文本编辑器
https://www.wangeditor.com/v5/getting-started.html#%E5%BC%95%E5%85%A5-js-%E5%88%9B%E5%BB%BA%E7%BC%96%E8%BE%91%E5%99%A8
```

# 2024-04-01

```
foxmail 邮箱密码： Bqk4TKknednBa9mw

centos8 升级内核
sudo dnf install https://www.elrepo.org/elrepo-release-8.el8.elrepo.noarch.rpm
sudo dnf config-manager --set-enabled PowerTools
sudo dnf config-manager --set-enabled elrepo-powertools
sudo dnf --enablerepo=elrepo-kernel install kernel-ml kernel-ml-devel
sudo dnf --enablerepo=elrepo-kernel install kernel-ml-tools

sudo grub2-set-default 0
sudo grub2-mkconfig -o /etc/grub2.cfg


echo "/usr/local/lib" | sudo tee -a /etc/ld.so.conf.d/libtasn1.conf  
sudo ldconfig

ldconfig -p | grep libtasn1

dnf install libgpg-error-devel

echo "/usr/local/lib" | sudo tee -a /etc/ld.so.conf.d/libksba.conf
sudo ldconfig
ldconfig -p | grep libksba
```

# 2024-04-07

```
myTV
# 查询dm状态
tiup dmctl --master-addr 192.168.15.225:8261 query-status test_cloud_merchant[merchant*]
# 删除dm同步任务
tiup dmctl --master-addr 192.168.15.225:8261 stop-task test_cloud_merchant[merchant*]
tiup dmctl --master-addr 192.168.15.225:8261 stop-task test_cloud_merchant[agency*]
tiup dmctl --master-addr 192.168.15.225:8261 stop-task test_cloud_common 
tiup dmctl --master-addr 192.168.15.225:8261 stop-task  test_cloud_merchant[work*]
```

# 2024-04-08

```
# 批量跳过有错误
dm_master=192.168.15.225:8261;for i in `tiup dmctl --master-addr $dm_master query-status |grep taskName|awk '{print $2}'|cut -d, -f1|cut -d'"' -f2`;do tiup dmctl --master-addr $dm_master handle-error $i skip ; done

寄生兽
```

# 2024-04-10

```
python manage.py startapp deploy

开发投产工单功能
检查dd备份失败原因
```

# 2024-04-11

```
docker run -d -p 9000:9000 --name my-php82-fpm-container  -v /home1/piwik:/home1/piwik  my-php82-fpm
docker run -d -p 9000:9000 --name my-php82-fpm-container  -v /home1/piwik:/home1/piwik tmtde/php82-fpm-pdo_mysql

-v /root/docker/php82-fpm/php_errors.log:/var/log/php-fpm.log
-v /root/docker/php82-fpm/php-config:/usr/local/etc 

RUN docker-php-ext-install mysqli pdo_mysql gd exif sockets zip bz2 mysqlnd

[root@ip-172-31-25-217 php82-fpm]# cat Dockerfile 
# 使用官方的PHP 8.2 FPM镜像作为基础镜像
FROM php:8.2-fpm-alpine

RUN apk add --no-cache zlib* && docker-php-ext-install mysqli pdo_mysql gd exif sockets zip bz2 mysqlnd
# 暴露用于PHP FPM通信的端口
EXPOSE 9000

# 启动PHP FPM服务
CMD ["php-fpm"]


docker build --no-cache -f Dockerfile_gd -t php82-fpm-mysql:v2 .


username = "matomo_user"
password = "QhK2vExGfCVvJ64G"
dbname = "matomo"
tables_prefix = "matomo_"

docker rm -f my-php82-fpm-container
docker run -d -p 9000:9000 --name my-php82-fpm-container  -v /home1/piwik:/home1/piwik -v     /root/docker/php82-fpm/php-config:/usr/local/etc/php  php82-fpm-mysql:v2

apk add zlib-dev libpng-dev
```

# 2024-04-12

```
# 搭建四方mysql   
# percona8安装   
# 下载地址 https://docs.percona.com/percona-server/8.0/quickstart-overview.html?_gl=1*1lualmo*_gcl_au*MjA4NTE2MjE4My4xNzEyOTAyMTI3*_ga*MTc4NTMxMzExMy4xNzEyOTAyMTM2*_ga_DXWV0B7PSN*MTcxMjkwMjEzNi4xLjEuMTcxMjkwMjIwMi42MC4wLjA.#purpose-of-the-quickstart

tar xvf Percona-Server-8.0.30-22-r7e301439b65-el7-x86_64-bundle.tar
yum localinstall percona-server-*.rpm percona-icu-data-files-8.0.30-22.1.el7.x86_64.rpm -y

systemctl start mysqld
systemctl enable mysqld
systemctl status mysqld

# 修改密码
grep 'temp' /var/log/mysqld.log  && mysql -uroot -p"$(grep 'temp' /var/log/mysqld.log | awk -F 'root@localhost:' '{gsub(/ /, "", $2); print $2}')"

alter user user() identified by WITH mysql_native_password 'hqZCj3n&S*gzxm4f';

ALTER USER 'root'@'%' IDENTIFIED WITH mysql_native_password BY 'hqZCj3n&S*gzxm4f';

# 安装 libev-4.15-7.el7.x86_64.rpm
# 下载地址 mirror.centos.org/centos/7/extras/x86_64/Packages/libev-4.15-7.el7.x86_64.rpm
yum localinstall -y libev-4.15-7.el7.x86_64.rpm
# 安装xtrabackup
tar xvf Percona-XtraBackup-8.0.30-23-r873b467185c-el7-x86_64-bundle.tar
yum localinstall -y percona-xtrabackup-80-8.0.30-23.1.el7.x86_64.rpm
# 安装qpress
yum localinstall -y qpress-11-3.el7.x86_64.rpm
```

# 2024-04-15

```
# 吉他谱下载
https://www.jitashe.org/artist/
```

# 2024-04-16

```
# 工单审核系统
192.168.15.216:9123    admin   EHMGPEa%5Uu%

1.日常邮件处理。完成情况：98%。邮件处理难免会有漏执行的情况，通过提醒以后执行完成。因此此项扣2分。
2. 部署灾备监控.完成情况：100%
3.解决DM同步故障.完成情况：100%
4.修复监控networker备份任务脚本.完成情况：100%
5.测试阿里云监控告警是否异常
6.修复备份检查脚本bug。完成情况：100%
7.开发投产工单功能.完成情况：100%
8,检查dd备份失败原因.完成情况：100%
```

# 2024-04-17

```
mkdir -p /root/docker/nginx/{conf,conf.d,html,log}
docker rm -f nginx
docker run --name nginx \
    -p 443:443 \
    -p 8088:8088 \
    -p 80:80 \
    -v /root/docker/nginx/conf/nginx.conf:/etc/nginx/nginx.conf \
    -v /root/docker/nginx/conf/sslkey:/etc/nginx/sslkey \
    -v /root/docker/nginx/conf.d:/etc/nginx/conf.d \
    -v /home1/piwik:/var/www/html \
    -v /root/docker/nginx/log:/var/log/nginx \
    -tid nginx

 docker exec nginx-container nginx -s reload
 docker restart nginx-container

 docker rm -f my-php82
 docker run -d -p 9000:9000 --name my-php82  -v /home1/piwik:/var/www/html -v /root/docker/php82-fpm/php-config:/usr/local/etc/php  php82-fpm-mysql:v2
```

# 2024-04-18

```
# redis
docker run --name redis -d -p 63379:6379 -e "REDIS_PASSWORD=8rb0OsKQuBCLvlkk" --restart always 056196042146.dkr.ecr.cn-north-1.amazonaws.com.cn/ipa/redis:latest

# 166创建数据盘
pvcreate /dev/xvdc
vgcreate dockervg /dev/xvdc
lvcreate -n docker_data -l 100%FREE dockervg
mkfs.xfs /dev/mapper/dockervg-docker_data 


# mysql   data目录需要27权限
docker rm -f mysql
docker run -tid --name mysql -p 33306:3306 \
-v /data/mysql:/var/lib/mysql  \
-v /data/etc/my.cnf:/etc/my.cnf \
--restart always \
container-registry.oracle.com/mysql/community-server:8.0.26

# skip-grant-tables

# FLUSH PRIVILEGES;
# ALTER USER 'root'@'localhost' IDENTIFIED BY 'PassW0rd_';
ALTER USER 'matomo_user'@'localhost' IDENTIFIED BY 'QhK2vExGfCVvJ64G';
   QhK2vExGfCVvJ64G
# 71.136.105.107

# php82-fpm
docker rm -f php
docker run -d -p 9000:9000 --name php  -v /home1/piwik:/var/www/html -v /root/docker/php82-fpm/php-config:/usr/local/etc/php --restart always php82-fpm-mysql:v2


# nginx
# mkdir -p /root/docker/nginx/{conf,conf.d,html,log}
docker rm -f nginx
docker run --name nginx \
    -p 443:443 \
    -p 8088:8088 \
    -p 80:80 \
    -v /root/docker/nginx/conf/nginx.conf:/etc/nginx/nginx.conf \
    -v /root/docker/nginx/conf/sslkey:/etc/nginx/sslkey \
    -v /root/docker/nginx/conf.d:/etc/nginx/conf.d \
    -v /home1/piwik:/var/www/html \
    -v /root/docker/nginx/log:/var/log/nginx \
    --restart always \
    -tid nginx


# 测试
curl https://partnersocialpre.intel.cn
docker exec -it mysql mysql -uroot -p

cat /data/etc/my.cnf

# 挂在数据目录
# mysql
mount /dev/nvme2n1 /data
# piwik
mount /dev/nvme1n1p2 /home1


# 自动挂在
vi /etc/fstab
/dev/xvdb2 /home1     xfs defaults 0 0
/dev/xvde /data    xfs defaults 0 0

# 调整docker显示
docker ps -a --format "table {{.ID}}\t{{.Names}}\t{{.Image}}\t{{.CreatedAt}}\t{{.Status}}"

vi /root/docker/php82-fpm/php-config/php-fpm.d/www.conf
;user = www-data
;group = www-data
user = nginx
group = nginx


# 检测本机IP
https://checkip.amazonaws.com/


# 236
# nginx 
# mkdir -p /root/docker/nginx/{conf,conf.d,log}
docker rm -f nginx
docker run --name nginx \
    -p 443:443 \
    -p 8088:8088 \
    -p 80:80 \
    -v /root/docker/nginx/conf/nginx.conf:/etc/nginx/nginx.conf \
    -v /root/docker/nginx/conf/sslkey:/etc/nginx/sslkey \
    -v /root/docker/nginx/conf.d:/etc/nginx/conf.d \
    -v /home1/web:/home1/web \
    -v /root/docker/nginx/log:/var/log/nginx \
    --restart always \
    -tid 056196042146.dkr.ecr.cn-north-1.amazonaws.com.cn/ipa/nginx:1.25


# mysql   data目录需要27权限
mkdir /root/docker/mysql -pv
chown -R 27.27 /data/mysql

docker rm -f mysql
docker run -tid --name mysql -p 33306:3306 \
-v /data/mysql:/var/lib/mysql  \
-v /root/docker/mysql/etc/my.cnf:/etc/my.cnf \
--restart always \
container-registry.oracle.com/mysql/community-server:8.0.26

# mysql 数据目录需要改成27 27 权限
2024-04-22T00:53:16.598144Z 0 [System] [MY-010116] [Server] /usr/sbin/mysqld (mysqld 8.0.26) starting as process 1
2024-04-22T00:53:16.601533Z 0 [ERROR] [MY-013276] [Server] Failed to set datadir to '/var/lib/mysql/' (OS errno: 13 - Permission denied)

165
ALTER USER 'root'@'localhost' IDENTIFIED BY 'PassW0rd_';
ALTER USER 'root'@'localhost' IDENTIFIED BY 'LFa46pZundn^';

# 逻辑卷迁移到别的地方
lvchange -ay /dev/data/mydata

aws ecr get-login-password --region cn-north-1 | sudo docker login --username AWS --password-stdin 056196042146.dkr.ecr.cn-north-1.amazonaws.com.cn

056196042146.dkr.ecr.cn-north-1.amazonaws.com.cn/ipa/redis:latest
```

```
docker exec -it mysql mysql

FLUSH PRIVILEGES;
ALTER USER 'root'@'localhost' IDENTIFIED BY '22jddX7f8eYJ';

docker exec -it mysql mysql -uroot -p"22jddX7f8eYJ"
```

# 2024-04-26

```
# 工单审核系统
192.168.16.83   admin   EHMGPEa%5Uu%


https://kimi.moonshot.cn/chat/collmbprdij88tde7t00?data_source=tracer&track_id=pbaes.KyhmDguFNqPdIPuEwzmcWAeuUhZLHJZSFCu7ieApdZP6Rt4YetqHhxOVyaawwmRmyoZMRAVLs5uqe1VBRltA8kwsv7ANWGVWFynUm3eioKl-22p4Fbj7jWGThIm_4QnSrIUKdnrYN-hcHgIceMDYuRAX-HkUshbWoXOd1jAYp9Vj7idj2Z98UVYR4OPbOiLw&utm_campaign=TR_6juWQH4h&utm_content=&utm_medium=B%E7%AB%99PC%E7%AB%AF%E5%B9%BF%E5%91%8A&utm_source=bilibili&utm_term=
```

# 2024-04-28

```
1.日常邮件处理。完成情况：98%。邮件处理难免会有漏执行的情况，通过提醒以后执行完成。因此此项扣2分。
2. 搭建四方mysql.完成情况：100%
3. 开发部署工单系统，工单数据库.完成情况：100%
4.开发投产工单功能.完成情况：100%
5.检查dd备份失败原因，修复脚本bug。完成情况：100%
6.工单审核系统的持续开发，替换编辑器，支持markdown语法，支持文件上传功能，支持返回功能。完成情况：100%
7.工单系统迁移到独立服务器上。完成情况：100%
8.开发统计功能，提取markdown内容中的表格部分，输出为独立csv文件。完成情况：100%
```

# 2024-05-06

```
# 下载歌曲
https://xiageba.com/
```

# 2024-05-07

```
# nginx 记录真实ip
log_format  main  '$http_x_forwarded_for - $remote_user [$time_local] "$request" '
                      '$status $body_bytes_sent "$http_referer" '
                      '"$http_user_agent"';
```

# 2024-05-09

```
# 批量跳过ddl错误
dm_master=192.168.15.225:8261;for i in `tiup dmctl --master-addr $dm_master query-status |grep taskName|awk '{print $2}'|cut -d, -f1|cut -d'"' -f2`;do tiup dmctl --master-addr $dm_master handle-error $i skip ; done

# 解决测试dm同步故障问题

echo "0 0 * * * root /root/bin/split_nginx_log" | sudo tee -a /etc/crontab
```

# 2024-05-11

```
# 磁盘总大小和已使用大小
round((node_filesystem_size_bytes{instance="10.80.16.73:9100",mountpoint!~".*/run/user/1000|.*/boot/efi|.*/boot|.*/run.*", fstype!="rootfs"})/1024/1024/1024, 1)

round((node_filesystem_avail_bytes{instance="10.80.16.73:9100",mountpoint!~".*/run/user/1000|.*/boot/efi|.*/boot|.*/run.*", fstype!="rootfs"})/1024/1024/1024, 1)


http://10.80.3.26:30009/login
Xdzf@666

http://10.80.3.47:9090
```

# 2024-05-13

```
# node_exporter  放到 /opt/node_exporter/node_exporter 目录
vi /etc/systemd/system/node-exporter.service

[Unit]
Description=https://prometheus.io
[Service]
Restart=on-failure
ExecStart=/opt/node_exporter/node_exporter --collector.systemd --collector.systemd.unit-whitelist=(sshd|nginx|docker|mysqld).service --collector.ntp --collector.ntp.server="10.83.1.17" --collector.ntp.server-is-local  --collector.ntp.local-offset-tolerance=1ms --collector.ntp.protocol-version=4  --collector.ntp.max-distance=3.46608s

[Install]
WantedBy=multi-user.target

systemctl restart node-exporter
systemctl status node-exporter
```

# 2024-05-14

```
192.168.15.216:9123    admin   EHMGPEa%5Uu%

dm_master=10.80.17.19:8261;for i in `tiup dmctl --master-addr $dm_master query-status |grep taskName|awk '{print $2}'|cut -d, -f1|cut -d'"' -f2`;do tiup dmctl --master-addr $dm_master handle-error $i skip ; done


name: cis_order_pay_info_pos_2024*
task-mode: all
is-sharding: true
shard-mode: pessimistic
ignore-checking-items: []
meta-schema: dm_meta
enable-heartbeat: false
heartbeat-update-interval: 0
heartbeat-report-interval: 0
timezone: ""
case-sensitive: false
collation_compatible: loose
target-database:
  host: 10.80.17.13
  port: 4000
  user: root
  password: root
  max-allowed-packet: null
  session: {}
  security: null
mysql-instances:
- source-id: paas-83
  meta: null
  filter-rules: []
  column-mapping-rules: []
  route-rules:
  - route-01
  expression-filters: []
  black-white-list: ""
  block-allow-list: balist-01
  mydumper-config-name: dump-01
  mydumper:
    mydumper-path: ./bin/mydumper
    threads: 4
    chunk-filesize: "64"
    statement-size: 0
    rows: 0
    where: ""
    skip-tz-utc: true
    extra-args: ""
  mydumper-thread: 0
  loader-config-name: load-01
  loader:
    pool-size: 16
    dir: ./dumped_data
    import-mode: sql
    on-duplicate: replace
  loader-thread: 0
  syncer-config-name: sync-01
  syncer:
    meta-file: ""
    worker-count: 16
    batch: 100
    queue-size: 1024
    checkpoint-flush-interval: 30
    compact: false
    multiple-rows: false
    max-retry: 0
    auto-fix-gtid: false
    enable-gtid: false
    disable-detect: false
    safe-mode: false
    enable-ansi-quotes: false
  syncer-thread: 0
  continuous-validator-config-name: validator-01
online-ddl: true
shadow-table-rules: []
trash-table-rules: []
online-ddl-scheme: ""
routes:
  route-01:
    schema-pattern: ds_cis
    table-pattern: cis_order_pay_info_pos_2024*
    target-schema: cis
    target-table: cis_order_pay_info_pos
filters: {}
column-mappings: {}
expression-filter: {}
black-white-list: {}
block-allow-list:
  balist-01:
    do-tables:
    - db-name: ds_cis
      tbl-name: cis_order_pay_info_pos_2024*
    do-dbs: []
    ignore-tables: []
    ignore-dbs: []
mydumpers:
  dump-01:
    mydumper-path: ./bin/mydumper
    threads: 4
    chunk-filesize: "64"
    statement-size: 0
    rows: 0
    where: ""
    skip-tz-utc: true
    extra-args: ""
loaders:
  load-01:
    pool-size: 16
    dir: ./dumped_data
    import-mode: sql
    on-duplicate: replace
syncers:
  sync-01:
    meta-file: ""
    worker-count: 16
    batch: 100
    queue-size: 1024
    checkpoint-flush-interval: 30
    compact: false
    multiple-rows: false
    max-retry: 0
    auto-fix-gtid: false
    enable-gtid: false
    disable-detect: false
    safe-mode: false
    enable-ansi-quotes: false
validators:
  validator-01:
    mode: none
    worker-count: 4
clean-dump-file: false
ansi-quotes: false
remove-meta: false
experimental:
  async-checkpoint-flush: false
```

# 2024-05·17

```
    server {
        listen       8088;
        server_name  _;
        root         /usr/share/nginx/html;

        location / {
            proxy_buffer_size 128k;
            proxy_buffers 32 128k;
            proxy_busy_buffers_size 128k;

            add_header Access-Control-Allow-Origin '*';
            add_header Access-Control-Allow-Methods '*';
            add_header Access-Control-Allow-Credentials true;
            set $auth 'Bearer eyJrIjoib2tSQ21JV2xQODV3bXRkbHlTUFlPQmdxM3luTlQxSlgiLCJuIjoiZGJhIiwiaWQiOjF9';

            proxy_set_header HOST $host;
            proxy_set_header Authorization $auth;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            proxy_pass http://grafana/;


<html> <head><title>414 Request-URI Too Large</title></head> <body> <center><h1>414 Request-URI Too Large</h1></center> <hr><center>nginx/1.20.1</center> </body> </html>
http {
    large_client_header_buffers 4 80k;
}
```

# 2024-05-21

```
192.168.15.216:9123    admin   EHMGPEa%5Uu%
```

# 2024-05-22

```
 # deploysubmit.html 模板中增加一个按钮
 <div class="form-group">
                            <button type="button" id="btn-save-draft" class="btn btn-warning">
                                保存为草稿
                            </button>
                        </div>


 # 增加处理按钮提交脚本
  <script>
        $("#btn-save-draft").click(function () {
            if (validateForm($("#form-submitdeploy"))) {
                document.getElementById('deploy_content').value = vditor.getValue();
                let formData = $('#form-submitdeploy').serializeArray().reduce(function (obj, item) {
                    obj[item.name] = item.value;
                    return obj;
                }, {})

                $.ajax({
                    type: "post",
                    url: "/api/v1/deployworkflow/",
                    dataType: "json",
                    contentType: 'application/json;',
                    data: JSON.stringify({
                            workflow: {
                                workflow_name: formData.workflow_name,
                                group_id: $("#group_name option:selected").attr("group-id"),
                                deploy_type: $("#deploy_type option:selected").attr("group-id"),
                                status: "workflow_draft",
                                workflow_id: sessionStorage.getItem('editWorkflowDetailId'),
                            },
                            deploy_content: formData.deploy_content
                        }
                    ),
                    complete: function () {
                        $('#btn-submitdeploy').button('reset')
                    },
                    success: function (data) {
                        window.location.href = "/deploy_detail/" + data.workflow_id
                    },
                    error: function (XMLHttpRequest, textStatus, errorThrown) {
                        if (XMLHttpRequest.responseJSON) {
                            alert(XMLHttpRequest.responseText)
                        } else {
                            alert(errorThrown);
                        }
                    }
                })
            }
    });
    </script>


# 在model中增加一个 choices
SQL_WORKFLOW_CHOICES = (
    ("workflow_draft", _("workflow_draft")),
   。。。
)


# 在api_workflow中增加处理逻辑
 @method_decorator(permission_required("sql.sql_submit", raise_exception=True))
    def post(self, request):
    。。。
        elif request.data['workflow']['status'] == 'workflow_draft':
            workflow_id = request.data['workflow']['workflow_id']
            if workflow_id:
                try:
                    instance = DeployWorkflowContent.objects.get(id=workflow_id)
                    serializer = DeployWorkflowContentSerializer(instance, data=request.data, partial=True)
                    serializer.is_valid(raise_exception=True)
                    serializer.update(instance, serializer.validated_data)
                except DeployWorkflowContent.DoesNotExist:
                    serializer.save()
。。。
```

# 2024-05-24

```
# 模型下载
https://huggingface.co/hfl/llama-3-chinese-8b-instruct-gguf/tree/main

# 模型微调
https://github.com/huggingface/peft

# 做量化
quantize

# 部署
ollama，lmstudio
ollama create 名字 -f Modelfile

# 本地知识库
docker pull phidata/pgvector:16
docker volume create pgvolume
docker run -tid -e POSTGRES_DB=ai -e POSTGRES_USER=ai -e POSTGRES_PASSWORD=ai -e PGDATA=/var/lib/postgresql/data/pgdata -v pgvolume:/var/lib/postgresql/data -p 5532:5432 --name pgvector phidata/pgvector:16

GROQ_API_KEY https://console.groq.com/keys 配置好自己的key这样就可以用70B的了

# 知识库向量化
ollama run nomic-embed-text

streamlit run app.py

python -m llama_cpp.server --model ./Yi-34B-Chat/ggml-model-Q3_K_M.gguf --n_gpu_layers -1 --n_ctx 2048 --chat_format chatml


# aws linux 安装crontab
sudo yum install -y cronie
sudo systemctl start crond
sudo systemctl enable crond
```

# 2024-05-27

```
# 去掉英文输入法
https://blog.csdn.net/yejiantao_0915/article/details/130342543

# 迁移docker数据
1.停止docker服务   systemctl stop docker
2.迁移数据
3.建立软链接
4.重启docker
```

# 2024-05-28

```
# gpt平替
https://cloud.dify.ai/
```

# 2024-05-29

```
https://github.com/wasp-lang/wasp
codegeex
rag
assistant api

文档加载切割
检索 es
向量检索 text embeddings

text-embedding-ada-002
chromadb 一个向量数据库
Milvus 比较出名的向量数据库
wget https://raw.githubusercontent.com/milvus-io/milvus/master/scripts/standalone_embed.sh
bash standalone_embed.sh start
fine-tuning
hugingface
PEFT 调优，迁移学习
LoRA
llama.cpp
# 4-bit量化
./quantize ./zh-models/33B/ggml-model-f16.bin ./zh-models/33B/ggml-model-q4_0.bin q4_0
# 加载启动模型
./main -m zh-models/33B/ggml-model-q4_0.bin --color -f prompts/alpaca.txt -ins -c 2048 --temp 0.2 -n 256 --repeat_penalty 1.1


# 加速ollama安装
vi /etc/hosts
# github 注意下面的IP地址和域名之间有一个空格
140.82.114.3 github.com
199.232.69.194 github.global.ssl.fastly.net
185.199.108.153 assets-cdn.github.com
185.199.109.153 assets-cdn.github.com
185.199.110.153 assets-cdn.github.com
185.199.111.153 assets-cdn.github.com

https://ollama.com/   
https://ollama.com/download/linux
curl -fsSL https://ollama.com/install.sh | sh
ollama run llava
ollama run llamafamily/llama3-chinese-8b-instruct
ollama pull smartcreation/dmeta-embedding-zh:f16
ollama list
dmeta
openwebui   https://github.com/open-webui/open-webui
OLMo

docker pull ghcr.dockerproxy.com/open-webui/open-webui:main
docker tag ghcr.dockerproxy.com/open-webui/open-webui:main ghcr.io/open-webui/open-webui:main
docker rmi ghcr.dockerproxy.com/open-webui/open-webui:main

docker run -d \
--network=host \
-v open-webui:/mnt/data1/ollama \
-e OLLAMA_BASE_URL=http://127.0.0.1:11434 \
--name open-webui \
--restart always ghcr.io/open-webui/open-webui:main
```

# 2024-06-05

```
192.168.15.216:9123    admin   EHMGPEa%5Uu%

aizh081*&!或者kunlun991

# 图形化展示训练结果
tensorboard --logdir=output

xxl_job 写个任务，每天执行
# paas的
CALL `ds_common`.`delete_push_record`();
CALL `ds_common`.`delete_push_history`();

# 行业的
CALL `cloud_common`.`delete_push_record`();
CALL `cloud_common`.`delete_push_history`();

D:\tasks\wxpusher\delete_push.py

# 本地模型下载部署测试
https://lmstudio.ai/

conda create -n mlx python=3.10
conda activate mlx
pip install mlx-lm

不方便下载模型的朋友可以私信发 【mlx】 自动获取模型和代码下载链接
https://ml-explore.github.io/mlx/build/html/index.html #MLX的文档 
https://huggingface.co/mlx-community #MLX社区，支持的模型可以在这里找到
https://github.com/ml-explore/mlx #苹果的MLX框架项目 
https://github.com/ml-explore/mlx-examples #MLX的示例代码，LoRA在此
https://pypi.org/project/mlx-lm/  #调用mlx模型的python API
https://www.anaconda.com/  #python虚拟环境工具
https://github.com/mewamew/mlx_demo#运行mlx模型示例代码

# langchain开发框架
https://python.langchain.com/v0.2/docs/introduction/

ALTER USER 'business_db'@'%' IDENTIFIED WITH mysql_native_password BY 'Ykvk8Z5R5DnVT_hUj28';

Langchain-Chatchat

# 删除虚拟环境
conda deactivate
conda env remove --name mlx

# 创建虚拟环境
conda create -n langchain python=3.11

# 安装部署 Langchain-Chatchat
# 拉取仓库
git clone https://github.com/chatchat-space/Langchain-Chatchat.git

# 进入目录
cd Langchain-Chatchat

# 安装全部依赖
pip install -r requirements.txt 
pip install -r requirements_api.txt
pip install -r requirements_webui.txt  

python copy_config_example.py
python init_database.py --recreate-vs
python startup.py -a

pip install -r requirements_lite.txt
python startup.py -a --lite
git clone https://www.modelscope.cn/qwen/Qwen1.5-14B-Chat-GPTQ-Int4.git 
```

# 2024-06-07

```
部署fastgpt
```

# 2024-06-11

```
# 部署fastgpt
mkdir fastgpt
cd fastgpt
registry.cn-hangzhou.aliyuncs.com/fastgpt/pgvector:v0.7.0
registry.cn-hangzhou.aliyuncs.com/fastgpt/mongo:5.0.18
registry.cn-hangzhou.aliyuncs.com/fastgpt/fastgpt-sandbox:v4.8.3
registry.cn-hangzhou.aliyuncs.com/fastgpt/fastgpt:v4.8.3
mysql:8.0.36 下载不下来
# 安装fastgpt
https://doc.fastgpt.in/docs/development/docker/
mkdir fastgpt
cd fastgpt
curl -O https://raw.githubusercontent.com/labring/FastGPT/main/projects/app/data/config.json

# pgvector 版本(测试推荐，简单快捷)
curl -o docker-compose.yml https://raw.githubusercontent.com/labring/FastGPT/main/files/docker/docker-compose-pgvector.yml
# milvus 版本
# curl -o docker-compose.yml https://raw.githubusercontent.com/labring/FastGPT/main/files/docker/docker-compose-milvus.yml
# zilliz 版本
# curl -o docker-compose.yml https://raw.githubusercontent.com/labring/FastGPT/main/files/docker/docker-compose-zilliz.yml

# 启动容器
docker-compose up -d
# 等待10s，OneAPI第一次总是要重启几次才能连上Mysql
sleep 10
# 重启一次oneapi(由于OneAPI的默认Key有点问题，不重启的话会提示找不到渠道，临时手动重启一次解决，等待作者修复)
docker restart oneapi

# 部署m3e
registry.cn-hangzhou.aliyuncs.com/fastgpt_docker/m3e-large-api:latest

# GPU
docker run -d -p 6008:6008 --gpus all registry.cn-hangzhou.aliyuncs.com/fastgpt_docker/m3e-large-api:latest
# CPU
docker run -d -p 6008:6008 registry.cn-hangzhou.aliyuncs.com/fastgpt_docker/m3e-large-api:latest

 # GPU模式执行
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) && curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo
yum clean expire-cache 
yum install -y nvidia-docker2
systemctl restart docker


# oneapi 中 
http://192.168.15.241:3001/    root/123456

http://192.168.15.241:6008
m3e
秘钥：sk-aaabbbcccdddeeefffggghhhiiijjjkkk


gpt-3.5-turbo
sk-kRvaGG3AVZqYUC2Wd67UYvB2OUGv54H4DxFsnQelVQAil1DW
```

# 2024-06-13

```
1.日常邮件处理。完成情况：98%。邮件处理难免会有漏执行的情况，通过提醒以后执行完成。因此此项扣2分。
2. 四方系统数据库重新部署.完成情况：100%
3. 工单系统的持续开发，数据库迁移.完成情况：100%
4.大屏dataease添加存储资源监控大屏.完成情况：100%
5.大模型的私有化部署测试。完成情况：100%
6.数据库服务器和存储厂商设备研讨会。完成情况：100%

m3e 的测试 api
curl --location --request POST 'https://localhost:6008/v1/embeddings' \
--header 'Authorization: Beaver sk-' \
--header 'Content-Type: application/json' \
--data-row '{
"model": "m3e",
"input": ["laf是什么"]
}'


https://app.nextchat.dev/#/?settings={"key":"sk-wvLZSQ5bvhlAjzr49c593eB0Cd054d5eB1073a829eC2F98e","url":"http://192.168.15.241:3001"}

sk-wvLZSQ5bvhlAjzr49c593eB0Cd054d5eB1073a829eC2F98e
http://192.168.15.241:3001

注意： 要把 "datasetProcess": true,  设置成true   
文本处理模型
```

# 2024-06-14

```
mysql 的其他备份文件信息如下：
备份路径：
10.80.16.86:/home/offline_dump
备份文件：
profit_real_time_payment_his_bak.sql.gz

# 语音 cpu  https://github.com/ahmetoner/whisper-asr-webservice
docker pull --platform linux/amd64 onerahmet/openai-whisper-asr-webservice

docker run -d -p 9000:9000 -e ASR_MODEL=base -e ASR_ENGINE=openai_whisper onerahmet/openai-whisper-asr-webservice:latest
docker run -d -p 9000:9000 -v c:/whiper/:/root/.cache/whisper -e ASR_MODEL=medium onerahmet/openai-whisper-asr-webservice:latest
# 可下载
docker run -d  -p 7860:7860 registry.gitlab.com/aadnk/whisper-webui:latest
```

# 2024-06-17

```
https://github.com/jhj0517/Whisper-WebUI
# 可下载
docker run -d  -p 7860:7860 registry.gitlab.com/aadnk/whisper-webui:latest
# 登录
http://192.168.15.241:7860/

 # deepseek
https://platform.deepseek.com/usage
sk-3b5a80fda2c04f9e80fa1b6e669207f1
https://api.deepseek.com
```

# 2024-06-18

```
# 单独下载包
pip download pipenv
pipenv requirements
pip download -r requirements.txt -d my_project_packages 

# 把venv创建到当前文件夹下
export PIPENV_VENV_IN_PROJECT=1
# 创建目录
mkdir test
# 进入目录，然后建virtualenv
pipenv install
# 激活
pipenv shell
# 查看虚拟环境
pipenv --venv

# 查看python的具体路径
import sys
print(sys.executable)
```

# 2024-06-24

```
运行一个本地模型文件 gguf
创建文件 Modelfile  里面包含模型文件的路径，如以下内容
FROM ./baichuan2-13b-chat.Q5_1.gguf

# 转换为ollama能识别的模型并取名叫 test_model
ollama create test_model -f Modelfile

# 运行交互界面
ollama run test_model

LLaMA-Factory

数据库                服务器
ds_capital  清结算（79）

dm_master=10.80.16.95:8261;for i in `tiup dmctl --master-addr $dm_master query-status |grep taskName|awk '{print $2}'|cut -d, -f1|cut -d'"' -f2`;do tiup dmctl --master-addr $dm_master handle-error $i skip ; done
```

# 2024-06-26

```
绩效
1.日常邮件处理。完成情况：98%。邮件处理难免会有漏执行的情况，通过提醒以后执行完成。因此此项扣2分。
2.私有大模型部署fastgpt部署，测试。完成情况：100%
3.工单系统的持续开发.完成情况：100%
4.数据大屏的持续改进优化.完成情况：100%
5.大模型应用到aliyun告警，运维应用探索。完成情况：100%
6.数据库服务器和存储厂商设备研讨会。完成情况：100%
7.部署delete_push_record等存储过程和定时执行任务。完成情况：100%

command+B并轨
option+k 键盘命令分配
铅笔指针
```

# 2024-07-01

```
# 手工清理awr数据
SELECT OCCUPANT_NAME, SPACE_USAGE_KBYTES / 1048576 "Space Used (GB)", SCHEMA_NAME, MOVE_PROCEDURE 
FROM V$SYSAUX_OCCUPANTS 
ORDER BY"Space Used (GB)" DESC;

SELECT dbid, min(snap_id), max(snap_id) FROM dba_hist_snapshot GROUP BY dbid;
      DBID MIN(SNAP_ID) MAX(SNAP_ID)
---------- ------------ ------------
2297956019      48904        49118


EXECUTE DBMS_WORKLOAD_REPOSITORY.DROP_SNAPSHOT_RANGE(low_snap_id => 48904, high_snap_id => 49118, dbid => 2297956019);

SELECT space_usage_kbytes
FROM v$sysaux_occupants
WHERE occupant_name = 'SM/AWR';

把cloud_order.channel_order_refund_inf_*表  同步到行业92，cloud_order库下


ALTER DATABASE DATAFILE '+DATADG/posp1/datafile/sysaux.263.992977581' RESIZE 30G;
```

# 2024-07-02

```
https://swanlab.cn/
pip install swanlab
swanlab login
5ButbV2gD46g2O6JB9vZJ

import swanlab
import random

# 初始化一个新的swanlab run类来跟踪这个脚本
swanlab.init(
  # 设置将记录此次运行的项目信息
  project="my-awesome-project",

  # 跟踪超参数和运行元数据
  config={
    "learning_rate": 0.02,
    "architecture": "CNN",
    "dataset": "CIFAR-100",
    "epochs": 10
  }
)

# 模拟训练
epochs = 10
offset = random.random() / 5
for epoch in range(2, epochs):
  acc = 1 - 2 ** -epoch - random.random() / epoch - offset
  loss = 2 ** -epoch + random.random() / epoch + offset

  # 向swanlab上传训练指标
  swanlab.log({"acc": acc, "loss": loss})

# [可选] 完成训练，这在notebook环境中是必要的
swanlab.finish()


我公司数据库用户账户仅限于授权人员，未使用的账户已被禁用或删除。所有数据库用户账户使用强密码策略，密码复杂度符合安全规范，且定期更换密码。
我公司已关闭数据库中的所有不必要的服务和端口，减少了潜在的攻击面。数据库实例配置已按照最佳实践进行，包括但不限于日志记录、审计功能和安全策略的实施。
我公司已根据最小权限原则分配数据库用户权限，确保用户只能访问其所需的最少数据和功能。定期审查用户权限和角色配置，确保权限没有不必要的扩展。
我公司制定并实施了完整的数据库备份和恢复策略，确保数据在任何意外情况下都能得到及时恢复。有定期备份和恢复过程测试，确保在实际需要时能够顺利执行。    
```

# 2024-07-11

```
# 创建并激活虚拟环境
conda create -n spleeter_env python=3.8 -y
conda activate spleeter_env

# 安装依赖项
conda install -c conda-forge ffmpeg libsndfile -y

# 安装 Spleeter
pip install spleeter

# 下载示例音频文件
wget https://github.com/deezer/spleeter/raw/master/audio_example.mp3

# 使用 Spleeter 分离音频文件
python3 -m spleeter separate -p spleeter:2stems -o output audio_example.mp3
```

# 2024-07-15

```
openssl pkcs12 -in wechat-bcaws.pfx -out wechat-bcaws.pem -nodes
intelintel123!


 GRANT SELECT ON `intel`.`ics_user_bind_log` TO `justin`@`%` 
```

# 2024-07-16

```
传输  uc网盘
https://fast.uc.cn/?entry=zhihu13

https://2.taobao.com/search/index.htm?q=3070ti&search_type=item&app=shopsearch
```

# 2024-07-18

```
# crontab 执行一次后自动删除
0 23 * * * /root/restart_docker.sh && (crontab -l | grep -v '/root/restart_docker.sh' | crontab -)



# 当mac安装完成的程序打不开的时候
xattr -d com.apple.quarantine 程序的路径
```

# 2024-07-24

```
# 在现有的项目上新建一个虚拟机环境用于开发
cd 到项目的根目录，执行
export PIPENV_VENV_IN_PROJECT=1
pipenv install
```

# 2024-07-26

```
1.日常邮件处理。完成情况：98%。邮件处理难免会有漏执行的情况，通过提醒以后执行完成。因此此项扣2分。
2.私有大模型部署fastgpt部署，测试。完成情况：100%
3.工单系统的持续开发.完成情况：100%
4.数据大屏的持续改进优化.完成情况：100%
5.大模型根据告警类型生成相应详情生成和解决方案提示。完成情况：100%
6.优化工单系统，去掉sql工单中多余的按钮。完成情况：100%
```

# 2024-07-30

```
优化工单系统，手工生成数据库列表
```

# 2024-08-05

```
持续优化工单系统，修复sql提交工单里面，实例列表无法显示问题
数据库默认显示请选择数据库提示
mv sqlsubmit.html sqlsubmit.html.fix && mv sqlsubmit.html.20240805 sqlsubmit.html

mv sqlsubmit.html sqlsubmit.html.20240805 && mv sqlsubmit.html.fix sqlsubmit.html
```

# 2024-08-06

```
# pip缓存清理
pip cache purge
conda create --name opsmanage python=3.10
conda activate opsmanage

pip install pipenv
pipenv --python 3.10
pipenv shell
```

# 2024-08-08

```
# 修复工单系统弹出权限错误的bug
# 流程图制作软件 drawio  可以结合gpt生成 mermaid 格式，然后粘贴到 drawio 里面
```

# 2024-08-19

```
账号    密码     密钥（输入到密码器，会生成MFA）
jinkong_luchan   CGk67afrrqf6n53NEIz   772THUAHNOMKVFBMSYYFQ6OOPI
jinkong_yinhb    5tz2A18uL3KhLiH1SLK   IDL4LGON5JQ3U3GTLQHZRHIY6M

堡垒机密码： Deny20080!!!
192.168.16.83   admin   EHMGPEa%5Uu%
```

# 2024-08-21

```
# 测试本机网络服务是否正常
nc -lk 12345
telnet 127.0.0.1 12345

阿里云  乐刷
jinkong@1105806183376752.onaliyun.com    1#XHYBzggL6r4ibcac9szz$^hq*6d3
账号    密码     密钥（输入到密码器，会生成MFA）
jinkong_luchan   CGk67afrrqf6n53NEIz   772THUAHNOMKVFBMSYYFQ6OOPI
jinkong_yinhb    5tz2A18uL3KhLiH1SLK   IDL4LGON5JQ3U3GTLQHZRHIY6M

# 乐刷数据库用户密码
域名 rm-wz98m7k7bh2h141kl.mysql.rds.aliyuncs.com
business_db  Nno4E1RRnYvTlUSGZ2gB
```

# 2024-08-26

```
# 
gunzip < backup.sql.gz | mysql -u username -p mydatabase > /dev/null 2>&1
# 
pigz -dc order_clearing_2022_7.sql.gz | mysql -h 10.80.16.96 -u root -p cloud_capital > /dev/null 2>&1

# 执行一次定时任务后删除， 需要执行脚本名称 /root/add_indexes.sh
(crontab -l 2>/dev/null; echo 'SCRIPT_PATH="/root/add_indexes.sh"'; echo '0 23 * * * $SCRIPT_PATH >> /root/add_indexes.log 2>&1 && (crontab -l | grep -v "$SCRIPT_PATH" | crontab -)') | crontab -
```

# 2026-08-27

```
clearing_separate_account_his_2024   清数据，然后重新同步24年的，已完成

account_flow_bkp_capital_his  已经做分表
act_accounting_entry   已经做分表
order_clearing_pos_fee   没有同步，也没做下线

# 做数据恢复
settle_info
split_settle_info
clearing_separate_account_his
```

# 2024-08-30

```
# 图片标注
pip install labelimg
# 输入命令
labelImg
```

# 2024-09-02

```
steam 账号 deny200801
```

# 2024-09-03

```
来客吧 数据库
账号   密码    密钥
jk-lkb_luchuan 02sZQs84LxqP20OZ2QxM V7IRS24XEGLDOG4MYBE6IE3NC4 
jk-lkb_yinhb Q0zP1OAfID0JL9noA28s 6HAZYHUXSQQTCFF5BPIXZAOEPA 

账户：jk-lka@1105806183376752.onaliyun.com
密码：hZng7aOkSr1(FiE^dj$DKMIe

1.日常邮件处理。完成情况：98%。邮件处理难免会有漏执行的情况，通过提醒以后执行完成。因此此项扣2分。
2.数据大屏的持续改进优化.完成情况：100%
3.乐刷数据库阿里云部署。完成情况：100%
4.优化工单系统，手工生成数据库列表。完成情况：100%
5.工单系统bug修复，修复工单系统弹出权限错误的bug。完成情况：100%
6.cmdb系统开发尝试。完成情况：100%


堡垒机密码： xajxuf-tuwko3-qaGbeg
192.168.16.83   admin   EHMGPEa%5Uu%
```

# 2024-09-04

```
# 表格图片数据集
curl -o PubTabNet.tar.gz https://dax-cdn.cdn.appdomain.cloud/dax-pubtabnet/2.0.0/pubtabnet.tar.gz

阿里云  乐刷金赢客
jinkong@1105806183376752.onaliyun.com    1#XHYBzggL6r4ibcac9szz$^hq*6d3
账号    密码     密钥（输入到密码器，会生成MFA）
jinkong_luchan   CGk67afrrqf6n53NEIz   772THUAHNOMKVFBMSYYFQ6OOPI
jinkong_yinhb    5tz2A18uL3KhLiH1SLK   IDL4LGON5JQ3U3GTLQHZRHIY6M

# 乐刷金赢客数据库用户密码rm-wz9joq0t2yk1dnz22.mysql.rds.aliyuncs.com
域名 rm-wz98m7k7bh2h141kl.mysql.rds.aliyuncs.com
business_db  Nno4E1RRnYvTlUSGZ2gB

乐刷来客吧 数据库
账号   密码    密钥
jk-lkb_luchuan 02sZQs84LxqP20OZ2QxM V7IRS24XEGLDOG4MYBE6IE3NC4 
jk-lkb_yinhb Q0zP1OAfID0JL9noA28s 6HAZYHUXSQQTCFF5BPIXZAOEPA 
域名： rm-wz9joq0t2yk1dnz22.mysql.rds.aliyuncs.com
账户：jk-lka@1105806183376752.onaliyun.com
密码：hZng7aOkSr1(FiE^dj$DKMIe
```

# 2024-09-06

```
python tools/infer/predict_system.py  --image_dir="/Users/mac/Desktop/小票/1725059218598.jpg" --det_model_dir="./ch_PP-OCRv3_det_slim_infer" --rec_model_dir="./ch_PP-OCRv3_det_slim_infer"
```

# 2024-09-10

```
# 安装自动标注
pip install PPOCRLabel

# 启动
PPOCRLabel

# 安卓投屏
brew install scrcpy
brew install android-platform-tools
# 启动
scrcpy

pip install ppocr-onnx
brew install android-platform-tools
```

# 2024-09-19

```
# 有效的手机截屏
result = subprocess.run(
                ['adb', '-s', '5cb045ad', 'shell', 'screencap', '-p'],
                stdout=subprocess.PIPE
            )
# 获取截图数据
screenshot_data = result.stdout

# 将截图数据转换为 OpenCV 图像
img_array = np.frombuffer(screenshot_data, dtype=np.uint8)
img = cv2.imdecode(img_array, cv2.IMREAD_COLOR)

# 显示图片
cv2.imshow("Screenshot", img)
cv2.waitKey(0)  # 按任意键关闭窗口
cv2.destroyAllWindows()
```

# 2024-09-21

```
https://netron.app

1.日常邮件处理。完成情况：98%。邮件处理难免会有漏执行的情况，通过提醒以后执行完成。因此此项扣2分。
2.乐刷来客吧阿里云部署.完成情况：100%
3.乐刷金赢客阿里云部署。完成情况：100%
4.优化工单系统，增加上传文件功能。完成情况：100%
5.乐刷系统上线，数据处理。完成情况：100%
6.云告警信息处理。完成情况：100%
7.机器学习方向摸索。完成情况：100%
```

# 2024-10-08

```text
pyqt6
pyside6-uic untitled.ui -o mydesign_ui.py
pyinstaller --noconsole -F ui_test.py -w
pyinstaller --noconsole -F main.py -w --icon=bzy.ico
pyinstaller -F scrape.py



```

# 2024-10-12

```text
微信小程序登录
https://mp.weixin.qq.com/

```

# 2024-10-15

```text
# yolo安装
pip install ultralytics


# cuda安装，https://developer.nvidia.com/cuda-toolkit-archive 或者从nvidia控制面板-系统信息-组件中可以看到
# 安装对应的toolkit
https://developer.nvidia.com/cuda-toolkit-archive

# 验证 nvcc命令不报错
nvcc --version


# 用x-anylabeling 用自己训练的模型标注数据，需要准备一个配置文件，注意，type是固定格式
type: yolo11
name: yolo11_test
display_name: test
model_path: C:/Users/Administrator/PycharmProjects/yolo_study/runs/detect/train/weights/best.onnx
nms_threshold: 0.45
confidence_threshold: 0.25
classes:
  - 商品信息

# github上安装ADBKeyBoard 可以解决安卓手机上粘贴中文文本问题

# 安装pytorch gpu版本
pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu124
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124


```

# 2024-10-16

```text
git config --global user.name  "Jack Yin"
git config --global user.email "oncwnuH6hYFHkXhmloGAMEY5QcP8@git.weixin.qq.com"

git clone https://git.weixin.qq.com/wx_wxddd06b1d87e9a2cf/deny20080.git
cd deny20080
touch README.md
git add README.md
git commit -m "add README"
git push -u origin master

cd existing_folder
git init
git remote add origin https://git.weixin.qq.com/wx_wxddd06b1d87e9a2cf/deny20080.git
git add .
git commit
git push -u origin master

登录用户名和密码： deny20080/De..

deny20080-2gfsa38r7ee3465b


wx.cloud.init({env: 'deny20080-2gfsa38r7ee3465b'});
const db = wx.cloud.database();

```

# 2024-10-21

```text
# 苹果键盘连接windows，需要输入PIN码解决办法：
苹果键盘连接windows，键盘上随便输个数字，然后点连接后快速的在无线键盘上输入同样的数字，回车就可以了

阿里云账号密码： 更新电话告警策略，取消30分钟未完结告警电话
31551158
xdjk6597507023

语音合成引擎安装
 pip install ttl
 
```

# 2024-10-22
```text
# 梳理conda开发，pycharm中如何设置
实验证明，使用了conda，pycharm中就老实使用conda避免出错

几个需要探索的网址
https://ai.bingal.com/cn/ai-tts/?utm_source=ai-bot.cn
# 音频合成
https://github.com/FunAudioLLM   
# 可以参考网页抓取
https://github.com/ihmily/DouyinLiveRecorder
# 屏幕录像网页插件
https://github.com/alyssaxuu/screenity

# ollama变量  OLLAMA_MODELS 配置模型保存的路径

```
# 2024-10-23
```
# pip安装pytorch gpu版本
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124


# conda安装pytorch gpu版本
conda install pytorch pytorch-cuda=12.4 -c pytorch -c nvidia

```
# 2024-10-24
```
好用的安卓投屏程序，可以支持键盘映射
https://github.com/barry-ran/QtScrcpy

# 用pyinstaller打包程序
常用参数： 
	-F 单文件 
	-c 命令行界面 
	-w windows界面，无命令行窗口 
	--add-data 缺什么补什么

pyinstaller -F --add-data "models/common.onnx;ddddocr" --add-data "screenshot;./screenshot" --noconsole 堡垒机登录.py


# 用这种方式保证打包后的路径正确性
def resource_path(relative_path):  
    """根据开发或打包环境获取资源路径"""  
    try:  
        # 打包后的环境: _MEIPASS 是临时解压路径  
        base_path = sys._MEIPASS  
    except AttributeError:  
        # 开发环境: 使用当前目录  
        base_path = os.path.abspath(".")  
  
    return os.path.join(base_path, relative_path)  
  
# 获取 icon_template_4.png 的绝对路径  
icon_template_path = resource_path('screenshot/icon_template_4.png')

```

# 2024-10-28
```
# amlogic usb burning 电视盒刷机工具
https://www.gizdev.com/latest-amlogic-usb-burning-tool/

# armbian  单板计算机系统
https://github.com/ophub/amlogic-s9xxx-armbian/releases

# 智能电视网 里面有很多电视需要的软件
https://www.znds.com/

motrix

梳理报警思路，将短信告警和阿里云，微信告警进行融合

```

# 2024-10-29
```
电视盒子CM311-5s 的cpu型号 国科 k6323v100c
# 智能电视网，下载电视盒子固件
https://www.znds.com/bbs-102-1.html

# 下载 FFmpeg
https://github.com/BtbN/FFmpeg-Builds/releases

# 翻墙 流量购买
https://www.easyfastcloud.com

# 无线密码   Xdjk@8888

```

# 2024-11-01
```
# 3ds 游戏下载

# windows 安装 choco 以管理员打开 powershell
Set-ExecutionPolicy RemoteSigned
iwr https://chocolatey.org/install.ps1 -UseBasicParsing | iex

```

# 2024-11-04
```
# torch 离线下载
https://download.pytorch.org/whl/torch/

# 绘制几何图形
https://www.geogebra.org/calculator
https://www.geogebra.org/download

```
# 2024-11-06
```
模型转换成 gguf 工具
# 克隆这个仓库
git clone https://github.com/ggerganov/llama.cpp.git
# 安装所需的依赖，测试用的3.10版本
# 执行转换脚本，一次成功
python convert_hf_to_gguf.py 源模型绝对路径 --outfile test.gguf

使用 lmstudio就可以运行

```
# 2024-11-07
```
# 直播源
https://yang-1989.eu.org/
# 手写风格的绘图
https://excalidraw.com/

# 山石客户端
https://www.hillstonenet.com/more/services/product-downloads/
# vpn 地址
https://118.123.206.128:4433

```

# 2024-11-12
```
# 英语单词数据
https://gitee.com/hoyin2020/wx_data/raw/master/data.json


```
##  flutter 多平台应用开发
***windows下不支持ios的应用开发*
## windows下环境配置
```
# sdk 下载地址
https://docs.flutter.dev/get-started/install/windows/mobile
https://docs.flutter.dev/release/archive

# 配置环境变量
D:\soft\flutter\bin
# 用户变量
FLUTTER_STORAGE_BASE_URL https://storage.flutter-io.cn
PUB_HOSTED_URL  https://pub.flutter-io.cn

# android studio command-tools 下载
打开android studio file->Settings->Languages & Frameworks->Android SDK->SDK Tools,从选项中勾选进行下载安装

# 从android studio的插件plugin中安装两个插件 dart, flutter

# 环境检查
flutter doctor

# 创建项目，在命令行终端
flutter create flutterdemo

vs 需要安装2个插件 dart 和 code runner
# 在所有终端上运行
flutter run -d all
flutter devices

# 下载依赖包
flutter pub get

# flutter 创建项目
flutter create english_assistant

# 如果vscode中flutter run出现乱码，则在终端中执行
chcp 65001

# chrome和驱动下载地址
https://googlechromelabs.github.io/chrome-for-testing/
# 查找包
https://pub.dev/

# 让元素实现在限制区域内滚动显示
SingleChildScrollView
Row 
mainAxisAlignment: MainAxisAlignment.center, // 使Row中的内容水平居中

# 看看Icons都有啥
https://fonts.google.com/icons

# 常用的获取主题字体样式，窗口的宽高
final txtTheme = Theme.of(context).textTheme;
final screenHeight = MediaQuery.of(context).size.height;
final screenWidth = MediaQuery.of(context).size.width;


```
## macos 配置
```
## 配置指南 https://docs.flutter.cn/get-started/install/windows/mobile
# 安装 vscode，安装flutter插件
# 安装 cocoapods （很慢）
sudo gem install cocoapods

# 下载flutter sdk
## vscode 中 打开命令行，输入 flutter ，选择 flutter:New Project,会弹出下载 flutter SDK

# 安装命令行工具
sudo sh -c 'xcode-select -s /Applications/Xcode.app/Contents/Developer && xcodebuild -runFirstLaunch'
sudo xcodebuild -license

# 安装ios模拟器
xcodebuild -downloadPlatform iOS
# 开启模拟器测试
open -a Simulator

配置环境变量 
.bash_profile,.zshrc
export PATH=$PATH:
export PUB_HOSTED_URL=https://pub.flutter-io.cn
export FLUTTER_STORAGE_BASE_URL=https://storage.flutter-io.cn

# 安装 android studio，安装flutter和dart插件，创建flutter项目

######### 下面的这个不需要设置，设置了反而有问题
# 修改android目录下 gradle-wrapper.properties 中的 distributionUrl
## 首先复制 distributionUrl 后面的链接下载，将下载下来的 gradle-8.3-all.zip 复制到android/wrapper下，然后修改distributionUrl路径为 ./gradle-8.3-all.zip
## 修改 build.gradle ，repositories 中添加 
google()
jcenter()
maven { url 'http://maven.aliyun.com/nexus/content/groups/public/' } //添加这一句

# 选择对应的终端设备，点运行即可运行


```
# 2024-11-19
在命令行中，输入code . 可以快捷打开vscode
```
# 下载依赖包
flutter pub get

# flutter 创建项目
flutter create english_assistant

# 如果vscode中flutter run出现乱码，则在终端中执行
chcp 65001

# chrome和驱动下载地址
https://googlechromelabs.github.io/chrome-for-testing/
# 查找包
https://pub.dev/

# 让元素实现在限制区域内滚动显示
SingleChildScrollView
Row 
mainAxisAlignment: MainAxisAlignment.center, // 使Row中的内容水平居中

# 看看Icons都有啥
https://fonts.google.com/icons

# 常用的获取主题字体样式，窗口的宽高
final txtTheme = Theme.of(context).textTheme;
final screenHeight = MediaQuery.of(context).size.height;
final screenWidth = MediaQuery.of(context).size.width;


```
# 2024-11-21
```python
# 免费apikey
API_URL="https://free.v36.cm"
API_KEY="sk-W2K6jeDgsgtXZY8L9b40EaFaF962489f945cA8Fc7bEeF0D2"

# gpt4 收费 apikey
sk-qcbnjf4lim11p1kA66DfF70dFbA84c2483Ba89Ec1237FdC1
接口地址：https://api.mixrai.com/v1/
客户端：https://app.nextchat.dev
余额查询网：https://key.mixrai.com

```

# 2024-11-26
```
# flutter 创建项目
flutter create english_assistant

# 如果vscode中flutter run出现乱码，则在终端中执行
chcp 65001

# chrome和驱动下载地址
https://googlechromelabs.github.io/chrome-for-testing/
# 查找包
https://pub.dev/

# 让元素实现在限制区域内滚动显示
SingleChildScrollView
Row 
mainAxisAlignment: MainAxisAlignment.center, // 使Row中的内容水平居中

# 看看Icons都有啥
https://fonts.google.com/icons

# 常用的获取主题字体样式，窗口的宽高
final txtTheme = Theme.of(context).textTheme;
final screenHeight = MediaQuery.of(context).size.height;
final screenWidth = MediaQuery.of(context).size.width;

```

# 2024-11-27
```
# 恢复开发环境官网数据库误删数据
# dm 225 测试环境同步故障处理

```

# 2024-11-29
```
# chrome和驱动下载地址
https://googlechromelabs.github.io/chrome-for-testing/
# 查找包
https://pub.dev/

# 让元素实现在限制区域内滚动显示
SingleChildScrollView
Row 
mainAxisAlignment: MainAxisAlignment.center, // 使Row中的内容水平居中

# 看看Icons都有啥
https://fonts.google.com/icons

# 常用的获取主题字体样式，窗口的宽高
final txtTheme = Theme.of(context).textTheme;
final screenHeight = MediaQuery.of(context).size.height;
final screenWidth = MediaQuery.of(context).size.width;
```
# 2024-12-03
```
# 让元素实现在限制区域内滚动显示
SingleChildScrollView
Row 
mainAxisAlignment: MainAxisAlignment.center, // 使Row中的内容水平居中

# 看看Icons都有啥
https://fonts.google.com/icons

# 常用的获取主题字体样式，窗口的宽高
final txtTheme = Theme.of(context).textTheme;
final screenHeight = MediaQuery.of(context).size.height;
final screenWidth = MediaQuery.of(context).size.width;


```
# 2024-12-04
```
# 证书操作
openssl pkcs12 -in certificate.fpx -nocerts -out private_key.pem
openssl pkcs12 -in certificate.fpx -clcerts -nokeys -out certificate.pem
openssl pkcs12 -in certificate.fpx -cacerts -nokeys -out chain.pem
cat certificate.pem private_key.pem > full_certificate.pem



```

# 2024-12-05
```
# sdk 下载地址
https://docs.flutter.dev/get-started/install/windows/mobile
https://docs.flutter.dev/release/archive

# 配置环境变量
D:\soft\flutter\bin
# 用户变量
FLUTTER_STORAGE_BASE_URL https://storage.flutter-io.cn
PUB_HOSTED_URL  https://pub.flutter-io.cn

# android studio command-tools 下载
打开android studio file->Settings->Languages & Frameworks->Android SDK->SDK Tools,从选项中勾选进行下载安装

# 从android studio的插件plugin中安装两个插件 dart, flutter

# 环境检查
flutter doctor

# 创建项目，在命令行终端
flutter create flutterdemo

vs 需要安装2个插件 dart 和 code runner
# 在所有终端上运行
flutter run -d all
flutter devices

# 下载依赖包
flutter pub get

# flutter 创建项目
flutter create english_assistant

# 如果vscode中flutter run出现乱码，则在终端中执行
chcp 65001

# chrome和驱动下载地址
https://googlechromelabs.github.io/chrome-for-testing/
# 查找包
https://pub.dev/

# 让元素实现在限制区域内滚动显示
SingleChildScrollView
Row 
mainAxisAlignment: MainAxisAlignment.center, // 使Row中的内容水平居中

# 看看Icons都有啥
https://fonts.google.com/icons

# 常用的获取主题字体样式，窗口的宽高
final txtTheme = Theme.of(context).textTheme;
final screenHeight = MediaQuery.of(context).size.height;
final screenWidth = MediaQuery.of(context).size.width;

# column 从左开始排版
crossAxisAlignment: CrossAxisAlignment.start,



```

# 2024-12-16
```
main 函数中 runApp 调用 MyApp 类，MyApp 是一个 StatelessWidget 类，在类中 build 方法返回 MaterialApp ，
MaterialApp 中 Home 定一个 StatelessWidget 或者 StatefullWidget 类， build 方法中返回一个 Scaffold
main -> runApp() -> MyApp -> return MaterialApp(home) -> Scaffold(body)
void main(){
	runApp(const MyApp());
}

class MyApp extends StatelessWidget {
	const MyApp({super.key})
	@overwrite
	Widget build(BuildContext context){
		return MaterialApp(
		home: const MyHomePage()
		);
	}
}

  final List<Widget> _screens = [
    HomePage(key: UniqueKey()),
    HistoryPage(key: UniqueKey()),
    TranslatePage(key: UniqueKey()),
    Container(key: UniqueKey()),
    Container(key: UniqueKey()),
  ];
  通过传入 key: UniqueKey() 可以保证点开新页面的时候重新初始化

```

# 2024-12-17
```
在 cmd 中，使用 start . 可以快捷打开当前目录文件夹
# flutter_sound
Failed to transform core-for-system-modules.jar to match attributes {artifactType=_internal_android_jdk_image, org.gradle.libraryelements=jar, org.gradle.usage=java-runtime}.

android->settings.gradle
plugins {
    id "dev.flutter.flutter-plugin-loader" version "1.0.0"
    // id "com.android.application" version "8.1.0" apply false
    id "com.android.application" version "8.7.0" apply false
    id "org.jetbrains.kotlin.android" version "1.8.22" apply false
}

android->gradle-wrapper.properties
# distributionUrl=https\://services.gradle.org/distributions/gradle-8.3-all.zip
distributionUrl=https\://services.gradle.org/distributions/gradle-8.9-all.zip

android\app\build.gradle
  defaultConfig {
  minSdkVersion 24
  
```

# 2024-12-18
```
pywebview
vnote
# 不好使，供参考
flutter run --machine --target=lib/main.dart --start-paused -d <device_id>
# flutter 降级，如果不知道降到具体多少，填一个比当前版本小一点的数字，执行命令后会有版本提示
flutter downgrade 3.7.0


```
# 20260119
```
esp32-s3



```


